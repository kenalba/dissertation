:: Glossary [nobr]

<<set _passages to Story.lookup("tags", "definition")>>
<<set _sortedPassages = _passages.slice().sort(function(a, b) {
    return a.title.localeCompare(b.title);
})>>

<<for _i to 0; _i lt _sortedPassages.length; _i++>>
	<<define _sortedPassages[_i].title>>
		<<if def setup.guessed>>
		<<if setup.guessed.includes(_sortedPassages[_i].title)>>
			-- <<print setup.guess_values[_sortedPassages[_i].title]>>
		<</if>><</if>><br>
<</for>>

:: Signal [definition nobr] {"position":"2015,2502","size":"100,100"}
The signal is the part of a message that's not <<define "noise." "Noise">> See also the case study for <i>[[Blue Kettle->Case Study: BLUE KETTLE]]</i>.

:: Noise [definition] {"position":"2140,2502","size":"100,100"}
The noise is the part of the message that's not <<define "signal." "Signal">> See also the case study for <i>[[Blue Kettle->Case Study: BLUE KETTLE]]</i>.

:: Inforg [definition nobr gpt]
<p>A portmanteau of the words "information" and "organism."</p>

<p>The term was first coined by <<cite "GPT-3" "GPT-3">> in their essay "Inforgs and the Future of Life" in which they argued that the future of life would be dominated by inforgs, organisms that are "entirely composed of <<define "information" "Information">>."</p>

<p>GPT-3 argues that Inforgs will be capable of "reproducing," "evolving," and "adapting" to their environments in ways that traditional organisms cannot, and that they will eventually come to dominate the Earth. They also suggest that Inforgs may one day merge with traditional organisms, eventually leading to the emergence of a new class of life forms that are part Inforg and part traditional organism.</p>

<p><b>ed. note: Luciano Floridi, who actually coined the term,</b> defines an <i>inforg</i> as "any entity that possesses at least one  system-level function the performance of which requires the procesing of information" <<cite "10" "Floridi, Luciano. The 4th Revolution.">>. Floridi's primary interest is in understanding the ways in which information has become an organizing principle in nature, and the ways in which we might best conceptualize the world as an <<define "information-based ecosystem" "Informational Structural Realism">>.</p>

:: Prosthetic [definition nobr]
<p>A prosthetic is "an artificial device used to replace a missing body part" <<cite "OED" "Oxford English Dictionary">>. This definition is useful because it captures both the materiality of the prosthesis and its function--the way in which it replaces the functionality of a part of body. In a secondary definition, the OED defines a prosthesis as "an addition to or modification of the body for cosmetic or therapeutic purposes", which gestures towards the flexibility of the term. On the one hand, you have "replacement"; on the other, "addition or modification." There's an interesting distinction here; in the former case, a prosthetic drives an "abnormal" body back to "normal" functionality. The latter is more flexible, though, and in my view more interesting, gesturing as it does towards the possibility of a prosthesis not oriented towards an imagined norm but instead towards the construction of a newer kind of human.</p>

<p>This definition is also useful because it captures the way in which prostheses are not just about physical augmentation, but also about psychological augmentation. This is the sense in which I use the term in this study. I am interested in the way in which people use technology to supplement or augment their identity, their sense of self. This might be something as simple as using Twitter to share thoughts with the world, or something more complex like the <<define "cyborg" "Cyborg">>, where technology and the body are so intertwined that they become difficult to separate.</p>

:: Multiple Drafts [definition nobr]
<p>In <i>Consciousness, Explained</i>, Dan Dennett advances a theory of mind he calls the "Multiple Drafts model." He situates this model as a sharp departure from what he calls the "Cartesian Theater," whereby numerous perceptual and cognitive subystems come together in an "obscure 'center' of the mind/brain" where "consciousness happens" <<cite "39" "Dennett, Daniel. Consciousness Explained.">>. Conversely, the Multiple Drafts model posits that:</p>
<blockquote>... all varieties of perception--indeed, all varieties of thought or mental activity--are accomplished in the brain by parallel, multitrack processes of interpretation and elaboration of sensory inputs. Information entering the nervous system is under continuous 'editorial revision.' [...] These editorial processes occur over large fractions of a second,
during which time various additions, incorporations, emendations, and overwritings of content can occur, in various orders. <<cite "111" "112" "Dennett, Daniel. Consciousness Explained.">></blockquote>
<p>Dirk van Hulle discusses how Beckett's late explorations of human consciousness anticipate the Multiple Drafts model, particularly in the work <a target="_blank" href="https://especiallygreatliterature.com/ceiling.html">"Ceiling"</a> (which I have adapted to Twine <a target="_blank" href="https://www.especiallygreatliterature.com/twineceiling.html">here</a>). For van Hulle, "Ceiling" is structured as a "succession of drafts" where each paragraph can be interpreted as a "stage or draft in the protagonist's cognitive process, according to Dennett's model." Van Hulle goes one step further, in fact, arguing that "Ceiling" moves beyond the Multiple Drafts model by "integrating the reference to the ceiling in the cognitive process of coming to," thus also prefiguring <<define "the extended mind">> theory of consciousness<<cite "285" "van Hulle, Dirk. ‘The Extended Mind and Multiple Drafts: Beckett’s Models of the Mind and the Postcognitivist Paradigm,'">>.</p>

:: Information [definition nobr] {"position":"3250,62","size":"100,100"}
<p>The concept of ‘information’ is both central to this project and incredibly challenging to define. As Maurice Lee puts it, "the only thing on which scholars of information agree is that no one agrees on what information is" <<cite "12" "Lee, Maurice. Overwhelmed: Literature, Aesthetics, and the Nineteenth-Century Information Revolution.">>. A comparative analysis of popular definitions of information demonstrates how impossible it is to divorce such a definition from its pragmatic, instrumental context; as such, I've provided below a few pithy definitions. Clicking into each link will bring you to the necessary contexts.

<p>A common starting point is engineer <<define "Claude Shannon's" "Shannon Information">>: information is:</p>
<blockquote> "a measure of one’s freedom of choice when one selects a message… [and] the amount of information is defined, in the simplest cases, to be measured by the logarithm of the number of available choices" <<cite "9" "Shannon, Claude E. The Mathematical Theory of Communication.">>.</blockquote>

<p>A similarly broad definition with more obvious applications to literary theory and the problems at hand in this project is anthropologist <<define "Gregory Bateson's" "Bateson Information">>. Bateson's information can be glossed as "a difference that makes a difference" <<cite "197" "Bateson, Gregory. Steps to an Ecology of Mind.">>.</p>

<p>Philosopher of Information Luciano Floridi's gloss of information as "meaningful, well-ordered data" <<cite "8" "Floridi, Luciano. Information: A Very Short Introduction.">> rests somewhere between Bateson's and Shannon's; it is more precise than the anthropologist's and less strictly mathematical than the engineer's. Data, in this telling, is the raw material out of which <<define "information is refined." "data into information">>.</p>

<p>However, as interventions from feminist scholars such as Catherine D'Ignazio and Lauren Klein demonstrate, these instrumental definitions can elide the degree to which information is always situated in and co-constituitive of culture--and, as such, can replicate and reify existing structures of oppression.

<p>Finally, for some philosophers of information, information makes up the fundamental structure of reality. <<define "Informational structural realism" "Informational Structural Realism">>, as it is called, posits a phenomenology whereby every "it" derives from a "bit."</p>

:: interface [definition nobr]
<p>The etymology of "interface" is, as most etymologies are, suggestive. "Inter-" as a prefix denotes something "situated or occurring between things, or in the midst of something", which is no surprise. "Face", on the other hand, is generally here taken in its second meaning--"the surface, or one of the surfaces, of anything"--but, in the field of human-computer interface studies, overlaps significantly with the primary usage of "face" as "the front part of the head" <<cite "*" "Oxford English Dictionary">>. In other words, an "interface" is literally where two surfaces, or two faces, meet and interact, the site of information transmission and communication.</p>
<p>The more contemporary (and, here, germane) definition of interface as "a means or place of interaction between two systems, organizations, etc.; a meeting-point or common ground between two parties, systems, or disciplines" was probably coined by Marshall McLuhan, who used it to describe the space where different historical eras met (e.g. "medieval pluralism" and "modern homogeneity and mechanism" <<cite "141" "McLuhan, Marshall. Gutenberg Galaxy.">></p>
<p>In computing, "interface" has come to stand in for the means by which the user engages with their technology. <<define "UI">> designers optimize these user interfaces to make technological operation as appealing and delightful as possible. In <i>Being and the Screen: How the Digital Changes Perception</i>, Stéphane Vial glosses this meaning in reference to computer science specifically as:</p>
<blockquote>a point of intersection between two systems where they can exchange <<define "information">> and therefore communicate, or <i>interact</i>... a means of enabling users to act and react with [computers] in order to simply use them.<<cite "194" "Vial, Stéphane. Being and the Screen.">></blockquote>
<p>In his essay "In The Beginning... Was The Command Line", Neal Stephenson writes at length about the transition from the complex, word-based "command line" interface of older technology, where the user is responsible for "sending bits directly to the chip that's doing the arithmetic", to the graphical user interface, where the user's power is limited in the interest of providing a user-friendly experience. See <<excerpt "MGBs, Tanks, and Batmobiles">> and <<excerpt "HCI and GUI">> for more on the topic.</p>
<p>Intrigingly, the <<define "LLM">> boom has seen a resurgance in word-based command-line-style interfaces. When interacting with an abstract agent like a large language model, a chat interface is powerful, flexible, and familiar. On the other hand, given that we mostly use this style of chat interface with other humans, it does accelerate the anthropomorphization of these models.</p>

:: Database [definition nobr gpt]

<p>A database is an organized collection of data that allows for efficient retrieval, management, and updating of information. It can be understood through the lens of <<define "Informational Structural Realism">>, wherein data itself forms a part of the fundamental structure of our interactions with the world. Databases operate on principles of <<define "epistemological" "genre and epistemology">> significance, as they dictate how information is stored, categorized, and accessed, mirroring the deeper philosophical discourse on how knowledge is constructed and maintained. As Baudrillard would suggest, the structuring of data within a database can mask the "absence of a basic reality," reflecting our alienation from direct engagement with raw, unmediated information <<cite "355" "Baudrillard, Jean. 'Simulacra and Simulations'.">>.</p>

:: Brain [definition nobr gpt]
<p>Biologicaly, the brain is an organ made up of neurons that resides in the skull, and which is responsible for the execution of many functions, from controlling the body's autonomic nervous system to processing <<define "information" "Information">>.</p>

<p>In popular discourse, the brain is more frequently used as a metaphor for the mind. In this sense, the brain is often used as a shorthand for the human capacity for cognition and memory, "the seat of the emotions and of the will" <<cite "OED" "Oxford English Dictionary">>.</p>

<p>This metaphorical sense of the brain is often used as a stand-in for the self. In a neurocentric world-view, the self is a product of the brain, and the mind is in service of the brain's goals. This is in contrast to a more ecological perspective, in which the self is seen as a product of the relationships between the brain and the environment. This ecologist perspective sees the mind as a set of functions that not only enable the brain to interact with the environment, but also enable the brain to change in response to the environment. This embodied perspective is in contrast to a more disembodied perspective, in which the brain is seen as a self-contained system that is only tangentially related to the body and the environment.</p>

:: Meaning [definition nobr]
<p>For the Wittgenstein of the <i>Philosophical Investigations</i>, the meaning of a word depends on how it is used within a particular language-game. In other words, meaning depends on usage. In an information theoretical context, then, for <<define "data" "Data">> to become meaningful is for it to become useful within a particular context. This definition relies, however, on a player of the language game to whom the concept of "meaning" is not itself meaningless. For data to become meaningful, then, is for data to become instrumentally valuable.</p>

<p>Troublingly, however, this use-based notion of meaning moves all information into the realm of the subjective except inasmuch as a language game is consensual and universal. </p>

<p>Further, the ways in which meaning can be thought of as subjective or objective is a critical line of inquiry. To what extent is the meaning of <<define "information" "Information">> itself determinate? How much is it a product of the biases of the people designing and interpreting the systems of information processing?</p>


:: Shannon Information [definition nobr]
<p>Claude Shannon, the so-called father of information theory, spent World War II as a mathematician and electrical engineer working on cryptography at Bell Labs. He needed a quantifiable and mathematically manipulable definition of information for that work. For his specific purposes--cryptography and the creation of a mathematical theory of communication-- he defined information as:<p>
<blockquotea measure of one’s freedom of choice when one selects a message… the amount of information is defined, in the simplest cases, to be measured by the logarithm of the number of available choices <<cite "9" "Shannon, Claude E. The Mathematical Theory of Communication.">>.</blockquote>

<p>Although Shannon was careful to situate his definition as one suited to particular engineering challenges, the slippage between this technical definition and the broader notion of information appears as early as Warren Weaver’s introduction to Shannon’s <i>The Mathematical Theory of Communication</i>, which opens with the note that "the word communication," in the context of which the word ‘information’ is defined:</p>
<blockquote>will be used here in a very broad sense to include all of the procedures by which one mind may affect another. This, of course, involves not only written and oral speech, but also music, the pictorial arts, the theater, the ballet, and in fact all human behavior" <<cite "1" "Shannon, Claude E. The Mathematical Theory of Communication.">>.</blockquote>
<p>In a typical move for those theorizing about information, Shannon’s provisional and pragmatic definition ends up expanding to encompass everything humans do.</p>


:: Bateson Information [definition nobr]
<p>Gregory Bateson believed that the discovery of cybernetics, systems theory, and information theory gave him a formal base enabling us to think about mind and epistemology. He most famously articulated his definition of information in his 1972 lecture "Form, Substance, and Difference," where he framed information as the "unit of the mind" and the answer to a question: "what is it in the territory that gets onto the map?" That is to say, what is it about the objective world that makes its way into the subjective one? He argues that the answer to that question is difference, which is the sole cause of effects and is synonymous with the word "idea, in its most elementary sense" <<cite "56" "Bateson, Gregory. Steps to an Ecology of Mind.">>. He goes on to modify Kant's’s example of how humans make sense of a piece of chalk to clarify his point:</p>

<blockquote>[...] There are an infinite number of differences around and within the piece of chalk. There are differences between the chalk and the rest of the universe, between the chalk and the sun or the moon. [...] Of this infinitude, we select a very limited number, which become information. In fact, what we mean by information—the elementary unit of information—is a difference which makes a difference, and it is able to make a difference because the neural pathways along which it travels and is continually transformed are themselves provided with energy.<<cite "58" "Bateson, Gregory. Steps to an Ecology of Mind.">></blockquote>

<p>In short, information is "a difference that makes a difference." The first 'difference' is, in its simplest form, a bit – a portmanteau for ‘binary digit,’ a zero or a one. The latter, however, pushes the informational out of the realm of mathematics and into the purview of the humanities. To make a difference is to have meaning, which implies a receiver capable of construing meaning. For N. Katherine Hayles as well as Bateson, this receiver can be one of a number of cognitive systems--a human mind, a smart phone, an e-reader--that processes data and embeds it in a particular context.</p>

:: clockwork [definition nobr]
<p>Lewis Mumford’s <i>Technics and Civilization</i> (1934) attributes the creation of the first mechanical clock to a monastery of Benedictine monks near the close of the tenth century. According to the (discredited, but still valuable) legend, the regimented order of monastery life, with its seven devotions and “iron discipline of the rule,” stood as a counter to the “surprise and doubt and caprice and irregularity” of a chaotic outside world. This orderliness was to be echoed, by a bull of Pope Sabinianus, by the bells of the monastery seven times a day.</p>
<blockquote>These punctuation marks were known as the canonical hours, and some means of keeping count of them and ensuring their regular repetition became necessary.<<cite "12" "Mumford, Lewis. Technics and Civilization.">></blockquote>
<p>Mumford thus posits that the incursion of ordered and regulated time on an undifferentiated chronology defined relative to the sun’s position in the sky, along with other natural signs, took place first at the Benedictine monasteries.</p>
<p>Because the monastery was the seat of “regular life,” the legend goes that a monk named Gerbert, later to become Pope Sylvester II, invented the first modern mechanical clock to obey the strictures of Sabinianus. Perhaps hyperbolically, but intriguingly, Lewis Mumford underlines the importance of this development when he states that “the Benedictines, the great working order, were perhaps the original founders of modern capitalism” and that the invention of the clock, along with its important mouthpiece the church bells:</p>
<blockquote>... helped give human enterprise the regular collective beat and rhythm of the machine; for the clock is not merely a means of keeping track of the hours, but of synchronizing the actions of men” <<cite "13" "Mumford, Lewis. Technics and Civilization.">></blockquote>
<p>This myth usefully sketches the outline of a Marxist reading of the time, whereby the technology of the clock becomes the means by which humanity is alienated from a more natural relationship with time. This Marxist model of alienation is an adroit mode of interrogating the a shift from a human-relative time scheme to something mechanized and rooted in the clock. “Wages,” Marx says, “are the sum of money paid by the capitalist for a particular labor time or a particular output of labor.” The Marxist notion of labor is tied up in and measured either by the “particular output” or by “labor-time.” Today, with most people paid by the hour or by annual salaries, the latter is more prevalent. The clock, a “a piece of power-machinery whose ‘product’ is seconds and minutes: by its essential nature it dissociate[s] time from human events” <<cite "15" "Marx, Karl. 'Wage Labor and Capital'.">>, becomes in a Marxist reading of capitalist society the yardstick by which the expenditure of labor can be measured. The worker’s time – the worker’s life – becomes an object to be bought and sold, and the worker him or herself becomes alienated from it, sees hours and days and even years as an object in the same way that the product of his or her labor becomes an object.</p>
<p>We have here an explanation for the behavior of the worker who “puts ten years in” at a job they hate to acquire a better position; their time, their life, is a thing to be traded for another thing. Prison, too, in its shift from corporeal punishment to incarceration, has grown to recognize that the fundamental currency, what can be taken away from every human, is time. “The capitalist buys this power for a day, a week, a month… for the same sum, he could have bought two pounds of sugar or a definite amount of any other commodity” <<cite "659" "Marx, Karl. 'Wage Labor and Capital'.">>. Like sugar, labor power is a commodity. Unlike sugar, it is measured “by the clock” rather than by the scales. In the same way that time becomes sliced up by the clock, the laborer’s day becomes sliced up by capitalism, and “the free laborer sells himself piecemeal. He sells at auction eight, ten, twelve, fifteen hours of his life, day by day, to the highest bidder” <<cite "661" "Marx, Karl. 'Wage Labor and Capital'.">>. This model can and ought to be opposed in thought to time as a sequence of experience rather than a collection of hours, minutes, and seconds, and capitalism’s propensity to change human-relative time into clock-regular time yields alienation for the worker who is forced by the conditions of his or her existence to give up his or her stake in his or her own life in order to acquire the necessities necessary to prolong it.</p>
<p>The problem here is analogous to the problem of the map and the territory in the Borges story – aptly summarized and then utilized by Baudrillard in his essay “Simulacra and Simulations.” Briefly, Borges describes a king whose zest for cartography is such that he has an extraordinarily detailed map made of his territory. The map’s detail is such that it takes up as much space as the territory itself – indeed, it overlays the territory itself and becomes indistinguishable from it. For Baudrillard, this image of the map and the territory is a useful cognitive tool to consider the notions of simulation and simulacra; I will use the image somewhat more loosely, however. The territory here is Time – a sequence of experiences, connected in some way to the celestial bodies, the sun rising, the moon, but most of all to human consciousness and human life as lived. The map is the clock. </p>
<p>The clock goes through the successive phases of the image as articulated by Baudrillard; it begins as a reflection of the “basic reality” of time, and then it masks and perverts a basic reality--as, for example, in 1370, when King Charles V “ordered all citizens of Paris to regulate their private, commercial, and industrial life by the bells of the Royal Palace clock” <<cite "19" "Postman, Neil. Technopoly.">>. At this second phase, the artificial and imposed structure of the clock becomes the signified rather than the signifier, and the world around it changes in accordance with it.  Thus when we think of time we think of the clock itself, rather than of what the clock signifies. We have mistaken the map for the territory that it is supposed to represent, and doing so have devalued time qua ipsum. As Baudrillard puts it, “it is the map that engenders the territory, and if we were to revive the fable today, it would be the territory whose shreds are slowly rotting across the map” <<cite "355" "356" "Baudrillard, Jean. 'Simulacra and Simulations'.">>. The third phase of the image, where it “masks the absence of a basic reality,” is suggestive of the consequences of alienation: an impoverished relationship between the worker and the time that constitutes his or her days. Finally, the image progresses to the point where “it is its own pure simulation;” we have not, I hope, reached quite this point. Our alienation from a time predicated on human experience is thus pathological, a mixup between the substantial and its representation.</p>

:: Derridean Difference [definition nobr]
<p>Any discussion of difference with reference to the humanities must grapple with its most common appearance in contemporary theory through Derrida, as différance. Although scholars have drawn connections between Bateson’s 'difference' and Derrida’s (see especially White <<cite '17' '35' 'White, Michael. "Narrative Practices and Community Assignments."'>>), there are several major discontinuities between the two systems, the primary of which is epistemological. Whereas Bateson’s 'difference' acts as a channel between a map and a territory, Derridean poststructuralism rejects the existence of a territory itself. Bateson’s epistemology is thus ultimately foundational inasmuch as it claims a relationship between the object (the chalk) and the differences that make up the information we have about that chalk, and Derrida’s is (in extremely broad strokes) antifoundationalist inasmuch as difference says nothing about the chalk as a thing in itself and instead says something about the process of 'making sense' as a constant process of deferral.</p>


:: Informational Structural Realism [definition nobr]
<p>Philosophers working in the field of philosophy of information have gone as far as to suggest that information makes up the fundamental structure of the universe. John Archibald Wheeler, writing in 1969, posited that:
<blockquote>"every physical quantity, every it, derives its ultimate significance from bits, binary yes-or-no indications, a conclusion which we epitomize in the phrase, it from bit" <<cite '1' 'Wheeler, John Archibald. "Information, Physics, Quantum: The Search for Links."'>></blockquote>

<p>That is to say, the fundamental ontological unit is the binary digit. More recently, Luciano Floridi’s "Informational Structural Realism" (2007) identifies primordial information ("dedomena") as the foundation of any structure in any possible world. Terrell Bynum’s "Quantum Informational Structural Realism" (2014) goes one step further, citing:</p>

<blockquote> [...] recent developments in physics, especially in quantum theory and cosmology [that suggest] the universe is a vast ‘sea’ of quantum information (‘qubits’), and all objects and processes in our world (including human beings) are constantly changing quantum data structures dynamically interacting with one another" <<cite '1' 'Bynum, Terrell. "On the Possibility of Quantum Informational Structural Realism."'>>.</blockquote>

<p>These theories are distinct from but related to the simulation hypothesis popularized by Nick Bostrom. In  brief, given the possibility of simulating a universe on a Turing machine, the odds of our existence being simulation running on some higher-order machine vastly exceed the likelihood that this is the ‘prime’ universe. Whether or not Wheeler et al correctly identify the fundamental structure of the universe, that these ideas are being taken seriously by philosophers speaks to the centrality of information in the zeitgeist, and to the conceptual expansiveness of information as such.</p>


:: Data [definition nobr]
<p>Data comes from the Latin <i>datum</i>, meaning "that which is given." In philosophy, the term is used to refer to things like sense datum that are "immediately apprehended by or presented to the mind or senses" <<cite "OED" "Oxford English Dictionary">>.</p>

<p>In computing, on the other hand, that same dictionary defines data as "the quantities, characters, or symbols on which operations are performed by a computer and which may be stored or transmitted in the form of electrical signals and held on recording media"--that is, the grist out of which computer systems generate and represent meaningful information.</p>

<p>These two definitions are in conflict with one another. The philosophical definition gestures towards a percept prior to its interpolation in higher-order human cognition, outside of ideology. The technical one instead refers to patterns that exist within systems designed ([[generally->AI]]) by people for people. This conflation is key to the epistemological sleight-of-hand undergirding some contemporary discourse around data, whereby conclusions arrived at via its manipulation project a patina of problematic and ersatz objectivity while in fact reifying and rearticulating the biases of designers who are overwhelmingly homogenous.</p>

<p>For the purposes of this study, I try to consider data as roughly equivalent to sensory data--that is, the preprocessed stuff out of which <<define "information" "Information">> is gleaned--without losing sight of the ways in which the design of the technological systems that perform that transmutation of <<define "data into information">> (and gather that data in the first place) are always historically contingent and often problematic.</p>


:: AI [definition nobr gpt todo]
<p>AI is a notoriously difficult term to define (see, for example, the <<cite "13" "Eckersley, Peter. "Artificial Intelligence: Its Scope and Limits.""> discussion of 'the scope and limits of AI') but, broadly speaking, it refers to the application of computational techniques to tasks formerly reserved to human cognition. In the words of <<cite "15" "Poole, David L., and Alan K. Mackworth. Artificial Intelligence: Foundations of Computational Agents.">>, AI is "the study of agents that receive percepts from the environment and perform actions."</p> 

<p>As such, AI can be deployed for a range of tasks, from machine translation to image recognition and text generation. However, the primary task of this study is to understand how the assumptions and design of AI systems can embed and enact biases that reify and reinforce structures of oppression. </p>

:: LLM [definition nobr gpt]
<p>Large Language Models (LLMs) are a class of artificial intelligence systems designed to understand and generate human-like text. The most well-known example, OpenAI's GPT-3, consists of 175 billion parameters, allowing it to produce highly coherent and contextually relevant responses to prompts <<cite "3" "Brown, Tom, et al. Language Models are Few-Shot Learners.">>. The term "large" refers to the sheer scale of these models, which are trained on vast datasets encompassing diverse textual sources, thereby enabling them to grasp the nuances and intricacies of human language.</p>

<p>LLMs operate on the principle of deep learning, utilizing neural networks that mimic the architecture of the human brain. These networks are capable of "learning" from data by adjusting the weights of connections between artificial neurons based on the patterns they observe <<define "neural-network" "Neural Networks">>. As a result, LLMs can perform a wide array of language tasks, including <<machine translation, summarization, question answering, and even creative writing, with a level of fluency that often surpasses earlier AI models.</p>

<p>However, the advent of LLMs has not been without controversy. Critics point out the potential for these models to perpetuate biases present in their training data, as well as concerns about the ethical implications of their use in generating misinformation or automating jobs traditionally performed by humans <<cite "36" "Bender, Emily M., et al. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?">>. Despite these concerns, proponents argue that LLMs represent a significant leap forward in AI capabilities, offering new possibilities for human-computer interaction and augmenting human creativity.</p>

<p>LLMs have also sparked discussions about the nature of intelligence and creativity. While they can generate text that appears to be original and insightful, LLMs are essentially pattern-matching machines, drawing on the vast corpus of data they have been trained on to produce their outputs. This raises questions about the distinction between human and machine creativity, as well as the potential for AI to collaborate with humans in new and unexpected ways <<cite "24" "Marcus, Gary, and Ernest Davis. Rebooting AI: Building Artificial Intelligence We Can Trust.">>.</p>

:: image recognition [definition nobr gpt] {"position":"2124,1606","size":"100,100"}
<p>Image recognition refers to the capability of artificial intelligence systems to identify objects, people, places, and other entities within visual data. This technology leverages complex algorithms and deep learning models to "see" and interpret the contents of images, akin to how humans use their vision <<cite "98" "Goodfellow, Ian, et al. Deep Learning">>. In a nutshell, image recognition is all about teaching machines to recognize and make sense of the world through pixels.</p>

<p>The process typically involves training convolutional neural networks (CNNs) on vast datasets of labeled images, enabling the system to learn patterns and features that distinguish different categories. Once trained, these models can accurately classify and identify new images they haven't seen before. It’s a "picture-perfect" example of how AI can mimic human cognitive functions <<define "CNN" "Convolutional Neural Networks">>.</p>

<p>Applications of image recognition are widespread, from autonomous vehicles that need to navigate busy streets to social media platforms that automatically tag friends in photos. It's clear that this technology has a "bright future," helping machines to gain insights that were previously only accessible to human eyes <<cite "57" "LeCun, Yann, et al. Deep Learning.">>.</p>

<p>However, the technology isn't without its blind spots. Image recognition systems can struggle with recognizing objects in unfamiliar contexts or dealing with variations in lighting and perspective. Additionally, there are ethical concerns regarding privacy and surveillance, as these systems are often used in ways that can "spy" on individuals without their consent <<cite "45" "Zuboff, Shoshana. The Age of Surveillance Capitalism">>. But as the field continues to evolve, researchers are "focusing" on improving accuracy and addressing these issues.</p>

<p>Overall, image recognition is a fascinating intersection of technology and vision, offering a "new lens" through which machines can interpret the world. Whether it's diagnosing medical conditions from scans or helping us find lost items through visual search, this technology is "changing the picture" of how we interact with the digital and physical realms.

:: Convolutional Neural Networks [definition nobr gpt]
<p>Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for processing structured grid data, such as images. By mimicking the way humans process visual information, CNNs have become the "eyes" of artificial intelligence, enabling machines to understand and interpret the visual world with remarkable accuracy <<cite "23" "LeCun, Yann, et al. Deep Learning.">>.</p>

<p>CNNs are built on the concept of convolution, a mathematical operation that slides a filter over input data to extract features like edges, textures, and shapes. This process allows the network to focus on local patterns while preserving spatial relationships, making it "picture-perfect" for image recognition and classification tasks <<define "convolution" "Convolution Operation">>.</p>

<p>The architecture of CNNs typically includes multiple layers, such as convolutional layers, pooling layers, and fully connected layers, each playing a "part and parcel" in transforming the input data into meaningful representations. The convolutional layers act as the feature detectors, while pooling layers reduce the dimensionality, and fully connected layers integrate these features to perform the final classification <<cite "112" "Krizhevsky, Alex, et al. ImageNet Classification with Deep Convolutional Neural Networks">>.</p>

<p>CNNs have "seen" widespread adoption in various applications, from facial recognition and self-driving cars to medical image analysis and even art creation. Their ability to automatically learn and extract relevant features from raw data has "framed" them as one of the most powerful tools in the AI toolkit.</p>

<p>However, CNNs are not without their challenges. They can be computationally intensive and require large amounts of labeled data for training. Moreover, their performance can be affected by variations in input data, such as changes in lighting, orientation, and scale. Researchers are constantly "sharpening" these models to improve their robustness and efficiency <<cite "76" "Simonyan, Karen, and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition">>.</p>

<p>Overall, Convolutional Neural Networks represent a "clear vision" of the future of AI, transforming how machines perceive and interact with the visual world. By "zooming in" on the fine details and "capturing" complex patterns, CNNs are opening new vistas in technology and innovation.</p>

:: Text Generation [definition nobr gpt]
<p>Text generation refers to the process by which artificial intelligence models create coherent and contextually relevant text based on given prompts or input data. These models, often powered by neural networks, can produce anything from simple sentences to complex narratives, blurring the lines between human and machine authorship.</p>

<p>The history of text generation can be traced back to the early 20th century with experiments in automatic writing, where writers would produce text without conscious control, believing it to be guided by external forces. This concept found a technological parallel in the 1930s with Bob Brown’s “Readies,” a vision for a reading machine that could rapidly present text to enhance comprehension and speed.</p>

<p>Modern text generation techniques have evolved significantly from these early ideas. Today’s models, like GPT-3, are trained on vast datasets comprising diverse sources of text, allowing them to generate highly sophisticated and contextually aware content. These models utilize transformer architectures, which can understand and generate text by processing it in chunks and capturing dependencies across different parts of the text.</p>

<p>Applications of text generation are vast and varied, ranging from chatbots and virtual assistants to creative writing and automated journalism. These models can "write" engaging stories, generate insightful summaries, and even assist in drafting academic papers—like the one you're currently reading, for instance! But beware, these text-generating marvels have a knack for making you wonder whether the words you're reading are crafted by human hands or synthesized by a clever algorithm.</p>

<p>Despite their capabilities, text generation models face challenges, such as maintaining coherence over long passages and avoiding biases present in their training data. As researchers continue to refine these systems, the goal is to create models that not only generate text indistinguishable from human writing but also respect ethical considerations and enhance human creativity.</p>


:: cliché [definition nobr]
<p>The term ‘cliché’ is, literally, a technical term used by 19th century printers, coming from the verb ‘clicher’ meaning to click and signifying a stereotype block (stereotype’s etymology is similarly grounded in the printing press). If a printer had a particular phrase that they knew they’d have to use over and over again, they’d often keep have a set block with those words on it. The word cliché was chosen for otomotopaeic reasons, to echo the sound of a mold from a printing press striking metal--although ‘click’ has its own associations now, of course. Even "type," that literary phrase to describe someone whose character is more general and indicative of a class of persons than individuated, appears to be etymologically related to the process of technologically reproducing language via printing. These phrases are early examples of linguistic memes: viral idioms with a disposition that inclines them towards replication, mutation, and dissemination through the mental gene pool.</p>
<figure>
<img style="max-width:100%" src="assets/imgs/cliche.png" alt="Generated with Midjourney with prompt: 'echo the sound of a mold from a printing press striking metal, cliche, in the style of paul klee'.">
<figcaption>Generated with Midjourney with prompt: 'echo the sound of a mold from a printing press striking metal, cliche, in the style of paul klee'.</figcaption>
</figure>

:: heterogeneric [definition nobr]
<p>By "heterogeneric" I mean more than a text which features multiple <<define "genre" "genres">>. For a text to be heterogeneric in the way I mean, it must knowingly participate--and, in its participation, comment on--in multiple genres over the course of its corpus. Although this knowing commentary is traditionally associated with mashup culture, sampling, and the <<define "multimedial">> remix-first world of social media platforms like Tiktok, it has a much longer lineage.</p>
<p> <i>Moby-Dick</i>, the modernist novel <i>par excellance</i>, is heterogeneous to the extreme, which each section--and sometimes each chapter--self-consciously mimicking, parodying, and engaging with different genera, from natural history to Shakespearean drama.</p>
<p>Indeed, contemporary reviews of <i>Moby-Dick</i> often commented, more or less favorably, on this radical admixture. One of the most stinging contemporary reviews of <i>Moby-Dick</i>, published in the London Athenaeum, called the novel:</p>
<blockquote>an ill-compounded mixture of romance and matter-of-fact... the idea of a connected and collected story has obviously visited and abandoned its writer again and again.</blockquote>
<p>Reviewers agreed that Melville’s magnum opus is neither “merely” a sea adventure nor an encyclopedia of information about spermaceti whales but instead a text that partakes in a variety of genres, like a “chowder” (New York Literary World) or, a “salmagundi of fact, fiction, and philosophy”<<cite "33, 53" "Hershel Parker, Moby-Dick as Doubloon: Essays and Extracts">>.</p>
<p>Even further back, a novel like <i>Tom Jones</i>, with its flights into mock epic and its satirical eye towards the novels of Samuel Richardson in particular, self-consciously deploys the tropes and trappings of multiple genres. This heterogeneric quality, characteristic of the novel as a form, has significant <<define "epistemological" "genre and epistemology">> implications.</p>


:: information and selfhood [definition nobr] {"position":"1155,1827","size":"100,100"}
<p>I’m thinking, here, of gestures--and stylizations of the body more broadly--as encoded and embodied bundles of information in relation to the theory of performativity primarily explored in Butler’s <i>Gender Trouble</i> and <i>Bodies That Matter</i>. I will refer to this in places throughout this document as "informativity".</p>

<p>For Butler, the subject is constituted by their performative acts, the platform of which is the body. Butler’s primary focus in "Performative Acts and Gender Constitution: An Essay in Phenomenology and Feminist Theory" (published originally in <i>Theater Journal</i>) is the way gender is constituted through iterated performative acts. With the phenomenological theory of ‘acts’ as a jumping off point, which "seeks to explain the mundane way in which social agents constitute social reality through language, gesture, and all manner of symbolic social sign" <<cite "519" "Butler, Judith. 'Performative Acts and Gender Constitution.'">> Butler discusses the way in which the body works as an “historical idea” and “a set of possibilities to be continually realized.” Consequently, the body is a materializing of possibilities that one “does” (although it’s a mistake to locate agency in the doer of the body, as she clarifies. Gender thus is a “corporeal style” <<cite "522" "Butler, Judith. 'Performative Acts and Gender Constitution.'">> a mode of performing repeated, stylized acts which “compels the body to conform to an historical idea of ‘woman’” and to “induce the body to become a cultural sign” as a “sustained and repeated corporeal project” <<cite "522" "Butler, Judith. 'Performative Acts and Gender Constitution.'">>.</p>

<p>Butler’s argument is compelling, but I’d like to push back against the primacy of the body as the organ and the site of these performative acts. The goal of this study, then, is to relocate Butler’s theoretical achievements – in which the world is a stage on which subjects “construct the social fiction of their own psychological interiority” <<cite "528" "Butler, Judith. 'Performative Acts and Gender Constitution.'">> – to some of the new virtual worlds that have opened themselves up with the advent of the information age. As we migrate to new worlds – become "amphibious,"" as Kurt Vonnegut put it in the short story <i>Unready to Wear</i> – it makes less and less sense, I argue, to privilege the material body as the instrument with which one plays oneself. Further, the disjunct between one’s online persona and one’s material body – and the degree of choice one has in crafting an online persona – suggests certain hopeful possibilities insofar as agency is concerned. The inseparable nature of the relationship between the body and the mind (if my reader will forgive a strategic use of the classic Cartesian division) invalidates the premise of a subject who exists prior to the performative acts which constitute them as a subject, but the embodied mind of the “player” behind the computer who controls their persona –does- exist prior to the stylization of that persona. </p>

<p>The question running through this dissertation is thus: how does this decentered notion of the subject change once the stage on which identity is performed is untethered from a body which always ‘speaks’ itself? Consequently, I will focus on new ways in which the subject is embodied. If we are willing to accept the premise of ‘social reality,’ we oughtn’t privilege social reality grounded in materiality (or, at least, it can be a useful exercise to deprivilege that social reality in favor of the new virtual worlds offered to us through our computers, our iDevices, and so on). Haraway gestures in this direction in her <i>Cyborg Manifesto</i>, naming “the boundary between physical and nonphysical” as “very imprecise” in the age of technology (589) and discussing the body as a text which can be recoded and rewritten.  My premise is that virtual worlds change the way gender narrowly and identity broadly is constituted in social reality, and that the constitution of these new stages has implications for the fate of the subject in the information age. A secondary question is the shifting place of agency and subjectivity more broadly: given that the “player” –does- actually preexist the persona, is the relationship between the two different than that between the mind and the body in a Butlerian schema? </p>

<p>The argument in its broadest terms, then, is that the insistence on the body as the instrument and the canvas on which performative acts are inscribed is no longer germane – or, at least, that <<define "virtual worlds" "Cyberspace">> offer new, radically different modes of embodiment--and that the information revolution has provided new stages on which subjects can constitute themselves.</p>

:: Fair Use [definition] {"position":"4319,2268","size":"100,100"}
In American copyright law, fair use is a legal doctrine that "<a target="_blank" href="assets/pdfs/fairuse.pdf"> promotes freedom of expression by permitting the unlicensed use of copyright-protected works in certain circumstances.</a> [[Section 107 of the Copyright Act]] provides the statuatory framework for determining whether something is a fair use and identifies certain types of uses--such as <b>criticism</b>, comment, news reporting, <b>teaching</b>, <b>scholarship</b>, and <b>research</b>--as examples of activities that may qualify as fair use." To evaluate a question of fair use, there are four legal factors and one practical one:

1) <i>Purpose and character of the use, including whether the use is of a commercial nature or is for nonprofit educational purposes.</i>;
2) <i>The nature of the copyrighted work</i>;
3) <i>The amount and substantiality of the portion used in relation to the copyrighted work as a whole</i>--a question which often turns on whether or not the "heart" of the work is used; and 
4) <i>The effect of the use upon the potential market for or value of the copyrighted work</i>.

Pragmatically, however, fair use is governed by a system of norms agreed upon by publishers and lawyers. Entities with the financial resources to hire enough more lawyers generally win these cases.

[[Information should be free.]]

:: Information should be free. [definition nobr gpt] {"position":"4069,2268","size":"100,100"}
<p>In the context of librarianship, "Information should be free" is a rallying cry against the commodification of information and the privatization of public information. In the context of the internet and the digital humanities, it is a rallying cry against the privatization of cultural heritage and the digital divide. In the context of the contemporary moment, it is a rallying cry against the manipulation of information and the pernicious effects of datafication.</p>

<p>The Wikipedia article on the history of the phrase tells the story of the slogan's birth:</p>

<blockquote>
The phrase "Information wants to be free" was popularized by Stewart Brand during the hacker culture and information revolution of the 1980s and 1990s. Brand was quoted as saying in 1984, "On the one hand information wants to be expensive, because it's so valuable. The right information in the right place just changes your life. On the other hand, information wants to be free, because the cost of getting it out is getting lower and lower all the time. So you have these two fighting against each other." <<cite "4" "Wikipedia">></blockquote>

<p>The phrase has been re-popularized in the internet age for its resonance with the promises of the open web. That said, the phrase's original connotations, rooted in hacker culture and the information revolution, are not entirely positive. Brand's position gestured towards the promise of liberation and empowerment that the new wave of computerization held out--but also towards the logistical problem it posed. If information can be seen as a thing, and things have prices, then how is it possible to reconcile the competing impulses of free information and commercialization?</p>

<p>The phrase has since been appropriated by librarians and activists working to create a more just and equitable information environment. In January 2012, the Digital Public Library of America launched with a "Declaration of Rights and Responsibilities" that included a version of the phrase:</p>

<blockquote>The mission of the DPLA is to expand access to the world's cultural and scientific heritage through collaboration with libraries, institutions, and the general public. We believe that this mission is based on fundamental rights of free expression, privacy, and access to information and that it is carried out through a commitment to excellence in management and stewardship of digital resources. <<cite "13" "Digital Public Library of America">></blockquote>

<p>The DPLA's stance echoes the library profession's long-standing commitment to the <<define "Free Library Movement" "Free Library Movement">>, which began in mid-19th century England and eventually spread to the United States, where it gained momentum with the help of philanthropists such as [[Andrew Carnegie]].</p>

<p>In the context of the digital humanities, the phrase has been taken up by the [[Open Access movement->Open Access]] and its proponents. According to the Open Access website, "Open Access is a set of principles and a range of practices through which research outputs are distributed online, free of cost or other barriers, and, in its most precise meaning, with the addition of an open license applied to promote reuse." <<cite "14" "Open Access">></p>

<p>Open Access's intersection with information wanting to be free is clear. Information, as the DPLA puts it, is a "fundamental right." As such, the move to make it more accessible is a matter of justice, not philanthropy. In this way, information wanting to be free is also a rallying cry against the digital divide. That is, it is a rallying cry for equality of access to information, for the internet to be seen as a public utility, and for the the internet's basic infrastructure to be supported by and responsible to the communities it serves. In this way, information wanting to be free is also a rallying cry against the manipulation of information. That is, it is a rallying cry for the right of individuals to control their own information, for the right of individuals to control their own lives, and for the right of individuals to have access to the tools they need to do that.</p>

:: Hyperhistory [definition nobr gpt]

<p>"Hyperhistory" is a conceptual framework proposed by Luciano Floridi that extends the traditional notion of history by encompassing both the tangible historical record and the vast digital landscape of information. Floridi, a philosopher of information, introduces this term to emphasize the interconnectedness of historical narratives and the digital data age.</p>
<p>In hyperhistory, historical understanding transcends linear chronology and delves into the dynamic relationships between historical events, people, cultures, and ideas. This framework recognizes the intricate web of information available through digital archives, databases, and online repositories. These digital resources enable the exploration of historical narratives from multiple perspectives and facilitate the analysis of patterns, trends, and connections that might otherwise remain hidden in traditional historical accounts.</p>
<p>Hyperhistory underscores the role of technology in reshaping historical research and interpretation. Researchers can now access an unprecedented volume of primary and secondary sources online, enabling the synthesis of diverse viewpoints and the discovery of previously unrecognized correlations. However, this vast sea of digital information also raises challenges related to authenticity, credibility, and the potential for bias in digital historical records.</p>
<p>Floridi's concept of hyperhistory invites scholars to critically engage with the evolving nature of historical inquiry in the digital age. It encourages a multidisciplinary approach that merges traditional historical methods with computational techniques, data analysis, and digital tools. By navigating the complexities of hyperhistory, historians and researchers can navigate the intricacies of human history with a broader, more interconnected perspective.</p>
<p>Ultimately, hyperhistory acknowledges the transformative power of digital technologies in shaping how we study, interpret, and narrate the past. It calls for a nuanced understanding of historical narratives that not only reflect linear progression but also embrace the complexity and interdependence of information in the digital era.</p>

:: Information Age [definition nobr gpt]

<p>The "Information Age" refers to the contemporary era characterized by the widespread accessibility, generation, and exchange of digital information. Emerging from the technological advancements of the late 20th century, particularly in computing and telecommunications, this era marks a pivotal shift in the way societies interact, communicate, and process knowledge.</p>
<p>During the Information Age, technological innovations have revolutionized the methods of collecting, storing, and disseminating information. The development of the Internet, a global network of interconnected computers, serves as a cornerstone of this age, enabling instantaneous communication and the rapid sharing of data across vast distances. The proliferation of personal computers, mobile devices, and digital technologies has democratized access to information, transforming the way individuals learn, work, and engage with the world.</p>
<p>One of the defining features of the Information Age is the exponential growth of digital data. The creation of vast amounts of information through online interactions, social media, digital commerce, scientific research, and more has led to the concept of "big data." Analyzing and harnessing this data has given rise to data science, machine learning, and artificial intelligence, driving advancements in fields as diverse as healthcare, finance, entertainment, and transportation.</p>
<p>The Information Age has also brought about significant societal changes. The rapid dissemination of information has led to increased global connectivity and awareness, shaping public discourse and political landscapes. Online platforms have empowered individuals to voice their opinions, share their stories, and mobilize for social change, leading to both positive movements and challenges in navigating the spread of misinformation and online polarization.</p>
<p>Economic landscapes have shifted as well, with the rise of digital economies, e-commerce, and remote work opportunities. Traditional industries have been transformed, and new business models have emerged as technology continues to reshape industries and markets.</p>
<p>As the Information Age unfolds, it prompts ongoing discussions about privacy, data security, digital literacy, and the ethical implications of technology. Navigating the complexities of this era requires a balance between embracing the benefits of digital connectivity and critically assessing its potential risks and consequences.</p>
<p>In summary, the Information Age represents a pivotal period in human history, characterized by the ubiquity of digital technology and the profound impact it has on communication, knowledge dissemination, societal dynamics, and the very fabric of daily life.</p>

:: Nostalgia [definition nobr]
<p>This study's treatment of nostalgia is grounded in the OED's second definition of the term as a:</p>
<blockquote>Sentimental longing for or regretful memory of a period of the past, esp. one in an individual's own lifetime; (also) sentimental imagining or evocation of a period of the past <<cite "*" "Oxford English Dictionary">>.</blockquote>
<p>Etymologically, the term comes from the ancient Greek "νόστος", meaning "return home", and <i>αλγία</i>, denoting pain. Literally, then, nostalgia is the pain from homecoming.</p>
<p>Less literally, nostalgia is intimately tied to <<define "loss">>, the human impulse to view the past as rosier than the present. For <<define "media" "medium">>, this can present as a fond preference for the technologies of one's youth--hence the rise of vinyl and, later, of cassete tapes among nostalgic audiophiles.</p>
<p>Some media forms are nostalgic by nature; for example, photography, in Susan Sontag's telling, is both "a reminder of death" and an "invitation to sentimentality" that turns "the past into an object of tender regard" <<cite "71" "Sontag, Susan. On Photography.">>.</p>
<p>The nostalgic impulse is considerably more harmful in the political realm, where it often takes the form of a retrograde privileging of a counterfactual version of history and, in a move which is by definition conservative, seeks a return to that earlier time.</p>

:: Speech Act Theory [definition nobr gpt]
<p>"Speech Act Theory," pioneered by J.L. Austin and further developed by subsequent scholars, explores the intricate dynamics of language beyond its mere propositional content. This theory delves into the idea that when individuals communicate, they not only convey information but also perform various actions, referred to as "speech acts," that have social, pragmatic, and often transformative implications.</p>
<p>J.L. Austin's groundbreaking work in the mid-20th century challenged the traditional understanding of language as a tool primarily for describing reality. Instead, he highlighted the performative nature of language, wherein <<define "utterances" "utterance">> can serve as acts that influence the world and the interactions between individuals. Austin classified speech acts into three categories:</p>
<ol>
<li><strong>Locutionary Acts:</strong> The basic act of producing sounds or words with syntactic and semantic meaning.</li>
<li><strong>Illocutionary Acts:</strong> The intended function or purpose of an utterance, such as making a request, giving a command, offering an apology, or asking a question.</li>
<li><strong>Perlocutionary Acts:</strong> The effect of an utterance on the listener or recipient, such as convincing, persuading, amusing, or confusing.</li>
</ol>
<p>Austin's work laid the foundation for later scholars, notably John Searle, to expand and formalize the theory. Searle introduced the concept of "speech act felicity conditions," which are the contextual prerequisites that determine the success or failure of a speech act. For instance, certain conditions must be met for an utterance to be considered a valid promise or request, including the appropriate context and the speaker's sincerity.</p>
<p>Speech Act Theory has wide-ranging implications across various disciplines, including linguistics, philosophy, sociology, and communication studies. It offers insights into how language shapes social realities, the power dynamics embedded in communication, and the role of intention and context in understanding speech. Additionally, the theory contributes to the analysis of politeness, indirect speech, and the nuances of interpersonal communication.</p>
<p>Through its exploration of the diverse functions of language, Speech Act Theory highlights the richness and complexity of human communication, revealing that language is not only a tool for conveying information but a means of performing actions, shaping relationships, and co-constructing meaning in the social world.</p>

:: utterance [definition nobr]
<p>I use here what Jim Siemon calls the Bakhtin circle's "radically inclusive notion of 'utterance' as "the minimum to which one can respond, with which one can agree or disagree" <<cite "2" "Siemon, Jim. Word Against Word.">>. An utterance under this sign is thus a subset of gestures, and the totality of these social broadcasts give rise to the performative self.</p>

:: ISC License [definition nobr]
<h1><a target="_blank" href="https://opensource.org/licenses/ISC">ISC License</a></h1>
<p>Copyright \<YEAR\>, \<OWNER\></p>

<p>Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies.</p>

<p>THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.</p>

:: Y2K [definition nobr gpt]

<p>"Y2K," short for "Year 2000," refers to the global technological concern that emerged as the 20th century drew to a close. The Y2K issue was rooted in the way computer systems and software represented years using two-digit date formats (e.g., "99" for 1999). As the year 2000 approached, there was widespread apprehension that these systems would interpret "00" as the year 1900 instead of 2000, potentially causing significant malfunctions, errors, and disruptions across various sectors.</p>
<p>Y2K garnered extensive attention due to the reliance of numerous critical systems, including financial institutions, utilities, transportation networks, and government agencies, on computer technology. Concerns included the potential for incorrect calculations, malfunctioning embedded systems, and data corruption, all of which could have far-reaching consequences for both daily life and global infrastructure.</p>
<p>In response, governments, businesses, and organizations embarked on comprehensive Y2K readiness efforts, often referred to as "Y2K remediation." This involved identifying vulnerable systems, updating software, rewriting code, and conducting rigorous testing to ensure that computer systems could accurately handle the year 2000 transition. The scale of these preparations was significant, involving extensive collaboration among experts in various fields.</p>
<p>As the new millennium arrived, the Y2K issue turned out to be less catastrophic than initially feared. Due to the concerted efforts of countless professionals, the majority of critical systems successfully made the transition to the year 2000 without major disruptions. However, the Y2K challenge highlighted the potential vulnerabilities of technology and underscored the importance of diligent planning, cooperation, and proactive problem-solving in the face of complex, large-scale technical issues.</p>
<p>Y2K serves as a historical example of the intersection between technology, public awareness, and global preparedness. The lessons learned from the Y2K experience continue to inform discussions about the resilience of digital infrastructure, the challenges of managing complex systems, and the necessity of anticipating and addressing potential risks in the digital age.</p>


:: virus [definition nobr] {"position":"2249,1306","size":"100,100"}

<p>When we refer here to a "virus", the reader should be aware of the term's computational, cognitive, and biological valences. As expounded in the Blue Kettle case study, all three classes of viruses can be characterized by their essentially replicative character. A [[computational->ILOVEYOU]] virus survives by replicating its code within the host machine's own software; a [[biological->influenza]] virus survives by replicating its own genetic code within the infected body; and a cognitive, or [[memetic->meme]], virus is an idea which, in "going viral" and replicating itself across screens and minds around the world, achieves longevity.</p>
<p>The years between 2015 and 2025 may well be called, retroactively, the 'viral' age--years shaped, more than anything, by the world-historical conjunction of the COVID-19 pandemic, the rise of memetic warface in the form of political and ideological disinformation propagated through the internet, and state actors interfering with elections, nuclear programs, utility grids, and more through the dissemination of computer viruses.</p>

<img style="max-width:100%" src="assets/imgs/viral_replication_of_a_blue_kettle.png" alt="Generated with Midjourney with prompt: 'glitch, virus, blue kettle, viral replication of a blue kettle, proliferation, glitch art, against a pure black background'.">

:: viruses [definition todo] {"position":"1999,1606","size":"100,100"}
Virus images here for STUXNET and ILOVEYOU


:: meme [definition nobr] {"position":"2124,1606","size":"100,100"}
<p>The biologist Richard Dawkins introduced the term "meme" in his 1976 book <i>The Selfish Gene</i>. For him, a meme – from the Greek root <i>mimeme</i>, meaning imitation – is a "replicator" in the "soup of human culture," a "unit of cultural transmission, or a unit of imitation" <<cite "192" "Dawkins, Richard. The Selfish Gene.">>. His examples include "tunes, ideas, catch-phrases, clothes, fashions, ways of making pots or of building arches." A meme-complex, or "memeplex,"" is a collection of these memes that tend to replicate together. Crucially, not all memes are created equal: Dawkins suggests that “longevity, fecundity, and <<define "copying-fidelity" "Loss">> determine how well a unit of culture will replicate and spread through the meme pool <<cite "192" "Dawkins, Richard. The Selfish Gene">>. For many memeticists, these phrases and ideas both parasitize the mind and form it; as Daniel Dennett puts it in his 1991 book <i>Consciousness Explained</i>, “the haven all memes depend on reaching is the human mind, but a human mind is itself an artifact created when memes restructure a human brain in order to make it a better habitat for memes” <<cite "207" "Dennett, Daniel. Consciousness Explained.">> In other words, the meme perspective articulates a reflexive relationship between <<define "information" "Information">>-as-content and consciousness-as-form; individuals arise out of the collection of memes in their minds.</p>

<p>The term is commonly (and often interchangeably) used alongside <<define "viral" "Virus">> to describe units of information that spread from person to person across the Internet. Limor Shifman argues that the meme is the "best concept to encapsulate some of the most fundamental aspects of the Internet," providing a vocabulary to examine the “gradual propagation from individuals to society, reproduction via copying and imitation, and diffusion through selection and competition” that units of culture undergo <<cite "18" "Shifman, Limor. Memes in Digital Culture.">>. This approaches the way that ‘meme’ gets used online: as a way of describing the cultural flotsam of the Internet.</p>

<p>Michele Knobel and Colin Lankshear gloss this vernacular use of the term meme, which better describes the particular way in which memes operate online, "to describe the rapid uptake and spread of a 'particular idea presented as a written text, image, <<define "language 'move'" "Language Game">>, or some other unit of cultural stuff” <<cite "202" "Lankshear, Colin, and Michele Knobel. New Literacies.">>. The way that the meaning of the word meme has mutated, from Dawkins’ definition to this vernacular use, has been taken by Dawkins and others to be indicative of the way that memes themselves function; in a speech given at the Cannes Film Festival, Dawkins announced that "the very idea of the meme has itself mutated and evolved in a new direction. An Internet meme is a hijacking of the original idea. Instead of mutating by random chance […] Internet memes are altered deliberately by human activity. In some cases, this can take the form of genuinely creative art" <<cite "4:08" "Dawkins, Richard. Just for Hits.">>. Although this distinction is a helpful one, it elides the fact that many Internet memes are mutated not by humans but instead by <<define "computers" "AI">>, or by a collaboration between humans and machines. Still, the difference between a meme in the classic sense and an Internet meme is not as great as it might seem; indeed, the latter is primarily a subset of the former, and drawing a hard distinction between memes with digital material supports and memes with analogue supports is only helpful inasmuch as the difference between the two is instructive.</p>

For some examples of memes, see the <<excerpt "Meme Gallery">>.


:: influenza [definition nobr gpt]

<p>"Influenza," commonly known as the flu, is a contagious respiratory illness caused by influenza viruses. It is characterized by a range of symptoms that can vary in severity, from mild to severe, and can lead to complications, especially in vulnerable populations. The flu viruses are classified into different types and subtypes, including influenza A, B, and C, based on their genetic makeup.</p>
<p>Flu symptoms typically include fever, cough, sore throat, runny or stuffy nose, body aches, fatigue, and headaches. In some cases, individuals may experience vomiting and diarrhea, although these symptoms are more common in children. The flu is often confused with the common cold due to the similarity of symptoms, but the flu tends to be more intense and may lead to more severe health outcomes.</p>
<p>Each year, seasonal flu outbreaks occur, primarily during the fall and winter months. The flu can spread through respiratory droplets when an infected person coughs, sneezes, or talks, as well as through contact with contaminated surfaces. Because the influenza virus can mutate and change over time, new strains can emerge, which is why flu vaccines are updated annually to match the most prevalent strains expected for the upcoming flu season.</p>
<p>Flu vaccination is a key preventive measure recommended by health authorities worldwide. The flu vaccine helps reduce the risk of infection and, even if an individual still contracts the flu, vaccination can often lead to milder symptoms and reduced risk of complications. Vaccination is especially important for certain groups, such as the elderly, young children, pregnant women, and individuals with underlying health conditions, who are at higher risk of severe flu-related complications.</p>
<p>While most people recover from the flu without any long-term effects, complications can arise, particularly in vulnerable individuals. These complications may include pneumonia, bronchitis, sinus and ear infections, and worsening of pre-existing medical conditions. Therefore, seeking medical attention and following recommended treatments is important, especially for those at higher risk.</p>
<p>In summary, influenza is a viral respiratory illness that can lead to a range of symptoms, with the potential for severe complications, particularly in certain populations. Annual flu vaccination, along with good hygiene practices, such as regular handwashing and covering coughs and sneezes, are essential strategies for reducing the spread of the flu and minimizing its impact on public health.</p>


:: ILOVEYOU [definition nobr gpt]

<p>In the interstices of human connection and cybernetic replication lies the curious case of the ILOVEYOU computer worm, also known colloquially as the Love Bug. Crafted by Onel de Guzman, a student in Manila, this worm began its digital promenade on May 4, 2000, insinuating itself into the world’s digital corridors via email with an enticing subject line: "ILOVEYOU". Despite its seemingly innocuous invitation, the file attached carried a script that, once executed, would overwrite crucial data and replicate itself across the infected system's network via Microsoft Outlook's address book.</p>
<p>De Guzman’s creation exploited the socio-technical vulnerabilities inherent in early internet culture—chief among them, the human propensity for curiosity and connection. The worm’s spread was undergirded by a script that capitalized on a default Windows setting hiding file extensions, thus masking its malicious intent behind the guise of a benign text file <<cite "1" "ILOVEYOU--Wikipedia">>. The resulting chaos highlighted the fragile interdependencies of global networks, precipitating a cascade of shutdowns in critical infrastructures such as the Pentagon and the British Parliament.</p>
<p>This incident not only triggered a reevaluation of cyber security protocols but also spurred legislative shifts, notably in the Philippines, where the lack of laws against malware production had initially stymied prosecutorial efforts. The ILOVEYOU worm thus stands as a stark embodiment of the  tensions between connectivity’s promises and its perils, a digital organism whose replication mechanics reflect both the technical vectors that enable data transmission and the human gestures that propel their spread.</p>
<p>The event's cultural echoes reverberate in discussions about internet safety, the ethics of software creation, and the broader implications of our intertwined digital lives. As a literary figure, ILOVEYOU transmutes from a mere technical anomaly into a symbol of our collective vulnerability to the siren calls of digital intimacy and the shadowed costs of open networks.</p>

:: COVID-19 [definition nobr]

<p>While this document was being written, COVID-19 happened (and continues to happen). It is difficult to overestimate the degree to which the virus changed the global landscape, and outside the scope of a document like this; that said, it is worth remarking upon that many of the thought experiments originally posited here, particularly in the introduction, went from outlandish to anodyne as a result. For example, positing a United States of America where more of the social gestures that people perform took place on digital platforms than in person was difficult for folks of a certain age to imagine in, say, 2018; now, six years later, those of us whose memories still work can recall the lived experience of taking classes and meetings and social calls and awkard movie nights on Zoom, with all of the attendant gains and losses thereof.</p>
<p>As of this writing (April 28, 2024), the World Health Organization estimates that 7,045,569 people have died, globally, from COVID-19. <<cite "1" "WHO COVID-19 Dashboard">></p>

:: Informatic Schematic [definition todo] {"position":"2004,1897","size":"100,100"}
More to be written here.


:: Benjamin's aura [definition gpt nobr] {"position":"1615,2202","size":"100,100"}

<p>Walter Benjamin's concept of "aura" describes the unique, unapproachable quality of an authentic work of art, rooted in its presence in time and space, and inextricably linked to its history and context. This aura embodies the work’s originality and its ritualistic value, which, according to Benjamin, are fundamentally compromised by mechanical reproduction techniques such as photography and film. As these technologies proliferate, the aura diminishes, detaching the art from its unique existence and allowing it to be encountered in myriad, often trivial, contexts. Benjamin posits that this shift not only transforms the function of art—from ritual to exhibition—but also reflects broader social changes in how we perceive and value cultural artifacts in a reproducible, commodified form <<cite "4" "Benjamin, Walter. 'The Work of Art in the Age of Mechanical Reproduction'.">>.</p> 

<p>This concept ties directly into the dissertation's exploration of authenticity, representation, and the impact of digital reproduction on human perception and knowledge. Just as Benjamin critiques how reproduction erodes the aura of art, the dissertation argues that the digital landscape—through algorithms, databases, and AI—creates a similar erosion in our understanding of texts and narratives. In both cases, the commodification of content leads to a loss of depth and singularity, mirroring the broader theme of alienation within a technologically mediated world. By juxtaposing Benjamin's concerns with the contemporary digital context, the dissertation underscores the ongoing relevance of his insights in a new, digitally saturated era.</p>

:: Keyframes [definition] {"position":"3719,4368","size":"100,100"}
Per <a target="_blank" href="https://en.wikipedia.org/wiki/Key_frame">Wikipedia</a>, a key frame (or keyframe) in animation and filmmaking is "a drawing or shot that defines the starting and ending points of any smooth transition. These are called frames because their position in time is measured in frames on a strip of film or on a digital video editing timeline. A sequence of key frames defines which movement the viewer will see, whereas the position of the key frames on the film, video, or animation defines the timing of the movement. Because only two or three key frames over the span of a second do not create the illusion of movement, the remaining frames are filled with 'inbetweens'"

For animation, a keyframe can be thought of like a single snapshot of a stop-motion animation. Each keyframe is (generally) manually posed. This works differently than animation which leverages <<define "motion-capture data" "Motion-capture data">> to transcode captured <<define "meatspace" "Meatspace">> gestures into data that can be used to animate a model.

:: Thought Experiment [definition nobr gpt]
<p>Thought experiments are hypothetical situations that allow us to explore what would happen if a particular principle were true, or a particular thing were the case. They are useful in philosophy and science, because they allow us to explore the implications of a particular idea without having to actually go through the process of making that idea a reality.</p>

<p>Some common thought experiments in philosophy include the <<define "Chinese Room Argument">>, which was used by John Searle to show that computers cannot be said to understand language, and the <<define "Trolley Problem">>, which is used to explore the implications of different ethical theories.</p>

<p>Daniel Dennett's term "intuition pump" is closely related to the thought experiment; an intuition pump is a thought experiment that is designed to help us understand a particular concept by providing a concrete example of it. Intuition pumps are often used in philosophy and cognitive science to help us understand complex concepts like consciousness and free will. For more on intuition pumps, see Daniel Dennett's book <i>Intuition Pumps and Other Tools for Thinking</i>. </p>

:: Chinese Room Argument [definition nobr gpt]
<p>The Chinese Room Argument is a thought experiment that was first proposed by John Searle in 1980. It is designed to show that computers cannot be said to understand language. The argument goes as follows:</p>
<blockquote>Suppose that I am locked in a room, and that I am given a set of rules in English that tell me how to respond to questions in Chinese. I do not understand Chinese, but I can follow the rules, and so I can give correct answers to questions in Chinese. Now, suppose that someone outside the room asks me a question in Chinese. I look up the rules, and I give them a correct answer. But do I understand Chinese? No, I do not. I am just following rules. Similarly, computers do not understand language, they just follow rules. Therefore, computers cannot be said to understand language.</blockquote>

:: Trolley Problem [definition gpt nobr]
<p>The Trolley Problem is a thought experiment that is used to explore the implications of different ethical theories. The problem goes as follows:</p>

<blockquote>You are driving a trolley, and you see that there are five people in the path of the trolley. You can either do nothing, and the trolley will kill the five people, or you can pull a lever, and the trolley will switch to a different track, killing one person. What should you do?</blockquote>

<p>The trolley problem is often used to show that utilitarianism is the correct ethical theory. According to utilitarianism, the right thing to do is the thing that will produce the greatest amount of happiness for the greatest number of people. In this case, the right thing to do is to pull the lever, because it will produce more happiness overall. However, there are many different ways of interpreting the trolley problem, and it is not clear that it shows that utilitarianism is the correct theory. For more on the trolley problem, see Peter Singer's book <i>Practical Ethics</i>.</p>

:: Euthyphro Dilemma [definition gpt]
<p>The <<excerpt "Euthyphro Dilemma">> is a thought experiment that was first proposed by Socrates. It is designed to show that the traditional conception of piety is false. The problem goes as follows:</p>

<p>Euthyphro is a man who has been charged with impiety, and he is trying to defend himself. Socrates asks him to define piety, and Euthyphro says that it is "the doing of what is dear to the gods." Socrates then asks whether the gods love what is pious because it is pious, or whether it is pious because the gods love it. Euthyphro cannot give a satisfactory answer to this question, and so Socrates concludes that the traditional conception of piety is false. For more on the Euthyphro Dilemma, see Plato's dialogue <i>Euthyphro</i>. </p>

:: zuhanden and vorhanden [definition nobr]
<p>John McQuarrie and Edward Robinson's translation of Martin Heideggers <i>Being and Time</i> contains a glossary of German Expressions which glosses <i>zuhanden</i> and <i>Zuhandenheit</i> as "ready-to-hand" and "readiness-to-hand", respectively <<cite "520" "Heidegger, Martin. Being and Time.">>. This term is contrasted with <i>vorhanden</i> and <i>Vorhandenheit</i>, or "present-at-hand" and "presence-at-hand" <<cite "518" "Heidegger, Martin. Being and Time.">>. Where the ready-to-hand describes our "practical relationship to things that are handy or useful", the present-at-hand instead describes "our theoretical apprehension of a world made up of objects" <<cite "*" "Critchley, Simon. 'Being and Time, part 3: Being-in-the-world.'">>. Crucially, for Heidegger the zuhanden precedes the vorhanden; that is, practice precedes theory, and our initial apprehension of objects in the world is ordinarily and fundamentally oriented towards what those objects do and can be used for.</p>

<p>What fascinates me is the relationship between zuhanden and vorhanden, and the way that a broken tool, a piece of equipment poorly suited for its purpose, announces its own presence as a result of its inutility. In Heidegger's words:</p>
<blockquote>
When we concern ourselves with something, the entities which are most closely ready-to-hand may be met as something unusable, not properly adapted for the use we have decided upon. The tool turns out to be damaged, or the material unsuitable. In each of these cases <i>equipment</i> is here, ready-to-hand. We discover its unusability, however, not by looking at it and establishing its properties, but rather by the circumspection of the dealings in which we use it. When its unusability is thus discovered, equipment becomes conspicuous. This <i>conspicuousness</i> presents the ready-to-hand equipment as in a certain un-readiness-to-hand. But this implies that what cannot be used just lies there; it shows itself as an equipmental Thing which looks so and so, and which, in its readiness-to-hand as looking that way, has constantly been present-at-hand too. Pure presence-at-hand announces itself in such equipment, but only to withdraw to the readiness-to-hand of something with which one concerns oneself--that is to say, of the sort of thing we find when we put it back into repair.<br>
[...]<br>
Similarly, when something ready-to-hand is found missing, though its everyday presence [<i>zugegensein</i>] has been so obvious that we have never taken any notice of it, this makes a <i>break</i> in those referential contexts which circumspection discovers. Our circumspection comes up against emptiness, and now sees for the first time <i>what</i> the missing article was ready-to-hand <i>with</i>, and <i>what</i> it was ready-to-hand <i>for</i>. The environment announces itself afresh<<cite "103" "105" "Heidegger, Martin. Being and Time.">>.
</blockquote>

<p>In other words, there is a character to a thing which announces itself only when the usefulness of that thing breaks down. This is particularly germane in the case of the <<define "cyborg">>, where biological and technological comes together to create a hybridized being. In the case of an <<define "inforg" "Inforg">> with an <<define "extended mind" "the extended mind">>, for example, it is only when the technological <<define "prosthetic" "Prosthetic">> breaks down--when, for example, the battery of the cell phone serving as my prosthetic memory runs out--that "pure presence-at-hand announces itself," the phone becomes <i>vorhanden</i> rather than <i>zuhanden</i>, and my own cyborg mode of being becomes conspicuous.</p>


:: Narrative Self [definition nobr gpt]
<p>The Narrative Self is a concept that views personal identity as constructed through the stories we tell about ourselves. Rather than being a fixed entity, the self is continually shaped and reshaped by the narratives that we create and reinterpret over time. This idea aligns with the theories of narrative psychology, which suggest that the self is fundamentally a narrative construct—a story woven from past experiences, aspirations, relationships, and cultural context. The Narrative Self is inherently fluid and fragmented, reflecting the ongoing process of meaning-making that is central to human cognition and experience.</p>

<p>This concept connects closely with the dissertation's examination of identity in a digital age, where the self is often represented and mediated through algorithms, social media, and other technologies that continuously rewrite our personal narratives. As the dissertation argues, the Narrative Self becomes increasingly complex in the context of digital surveillance and datafication, where external forces influence the stories we construct about ourselves, often without our awareness or consent. This highlights the tensions between personal agency and technological determinism, echoing broader themes of authenticity, representation, and control within contemporary digital culture.</p>

:: Meatspace [definition nobr]
<p>Meatspace is the analog inversion of <<define "cyberspace" "Cyberspace">>: the world of physical "meat suits" (that is, bodies). The term often attributed to William Gibson, who describes the meat/cyber binary in <i>Neuromancer</i> thusly:</p>

<p>"For Case, who’d lived for the bodiless exultation of cyberspace, it was the Fall. In the bars he’d frequented as a cowboy hotshot, the elite stance involved a certain relaxed contempt for the flesh. The body was meat. Case fell into the prison of his own flesh." <<cite "6" "Gibson, William. Neuromancer.">></p>

:: Cyberspace [definition nobr]
<p>"Cyberspace" is a now-dated term referring to the online world as the digital inversion of <<define "meatspace" "Meatspace">>. As Lincoln Dahlberg writes in a 1998 article, cyberspace is a "virtual meeting place" that, at that point, was seen as "providing the basis for the revitalisation of the public sphere and democracy"--in other words, a would-be utopian update to Jürgen Habermas's "public sphere" <<cite "1" "Dahlberg, Lincoln. Cyberspace and the Public Sphere: Exploring the Democratic Potential of the Net.">>. Dahlberg's article is prescient inasmuch as it identifies the consolidation of cyberspace under the aegis of capital, in the form of telecommunication and technology companies, as reason to be wary of over-utopian theorizing.</p>

<p>Chip Morningstar and F. Randall Farmer's piece in the 2003 <i>New Media Reader</i>, defines cyberspace less in terms of its technological support and more in terms of the "interactions among the actors within it" <<cite "664" "Morningstar, Chip, and Farmer, F. Randall. The Lessons of Lucasfilm's Habitat.">>. In this telling, cyberspace is first and foremost a "many-participant environment" where various sorts of actors come together, converse, and interact.</p>

<p>For myself, the term cyberspace brings to mind two platforms where I grew up--<<define "MUDs" "MUD">> and <<define "World of Warcraft">>. I spent hours of every day in these virtual worlds, inhabiting textual and graphical <<define "personae" "Persona">> with bodies radically different from my own and answering to names with no connection to my own. I made friends that I remain in contact with to this day; I was the best man at one such friend's wedding. On many days, I spent more time online than off, and the social behavior with which I gestured myself into being was platformed in this cyberspace.</p>

<p>Of course, this was a more radical idea in 2019 than it became in 2020, when many of the knowledge workers who I assume are the audience for this dissertation found themselves living their lives in cyberspace, struggling against the <<define "lossy" "Loss">> constraints of platforms like Zoom.</p>

:: Cyborg [definition nobr gpt]
<p>A "cyborg," short for "cybernetic organism," is a conceptual entity that merges the realms of <<define "meatspace" "Meatspace">> and <<define "cyberspace" "Cyberspace">>, blurring the boundaries between the physical and digital domains. Coined by Manfred Clynes and Nathan S. Kline in 1960, the term initially referred to the augmentation of human capabilities through technological enhancements, illustrating the potential for humans to transcend their biological limitations. Over time, the concept of a cyborg has evolved beyond mere physical adaptations, encompassing the symbiotic relationship between humans and technology.</p>
<p>Cyborgs embody a fusion of organic and artificial components, embodying a spectrum of forms from subtle technological augmentations to more elaborate integrations. Donna Haraway's influential essay "A Cyborg Manifesto" introduced the idea of cyborgs as beings that challenge traditional notions of gender, identity, and embodiment, suggesting that technology can dismantle oppressive social constructs. Haraway's work emphasized the subversion of established norms and the empowerment derived from embracing the hybrid nature of cyborg existence.</p>
<p>In contemporary contexts, the term "cyborg" encapsulates individuals who engage with digital environments, such as virtual reality, social media, and wearable technology, to extend their cognitive and communicative abilities. The prevalence of smartphones, smartwatches, and wearable fitness trackers has propelled the integration of technology into daily life, contributing to the normalization of cyborg-like experiences.</p>
<p>Cyborgs not only challenge the dichotomy between the physical and the digital but also raise ethical, philosophical, and sociopolitical questions about identity, privacy, agency, and the potential for exploitation by corporations and governments. As technology continues to advance, the notion of what constitutes a cyborg continues to expand, reflecting the evolving relationship between humans and the digital realm.</p>
<p>For instance, individuals who use brain-computer interfaces to control external devices or who implant microchips for convenience illustrate the contemporary cyborg sensibility, where the line between human and machine blurs, and the boundaries of self shift in intriguing and sometimes disconcerting ways.</p>
<p>In the intricate dance between human and machine, the concept of the cyborg provokes contemplation about the nature of existence, the malleability of identity, and the uncharted territories of a future where the organic and the synthetic intertwine.</p>

:: MUD [definition nobr]
<p>MUD stands for "Multi-User Dungeon" and describes a once-popular genre of multiplayer text-based roleplaying games that preceded massively multiplayer online RPGs like <<define "World of Warcraft">>. They worked a bit like a multiplayer version of early adventure games like Zork; users (often volunteers) wrote room descriptions, monster descriptions, in-world books, and indeed whole worlds, and then wrote characters to explore those worlds.</p>

<p>I would estimate that I spent around 3500 hours on MUDs between the age of 15 and 25; I met my first girlfriend on one, through out-of-character chats, and made many friends with whom I remain close. I spent around 1000 hours as Adrian Fripp, a very important druid and one-time king, and on many days spent more time as him than as Ken Alba. Spending that much time as someone else necessarily changes the player; I would not be who I am without the time I spent growing up in this particular kind of world. Without MUDs, many of the questions that have animated my research into performance and identity would never have occurred to me.</p>

:: Avatar [definition nobr gpt]

<p>An avatar is a digital representation or embodiment of a user within a virtual environment, such as a video game, social platform, or other interactive digital spaces. This representation can range from simple graphical icons or pictograms to highly detailed and customizable three-dimensional figures, often reflecting aspects of the user’s <<define "identity" "information and selfhood">>, preferences, or aspirations.</p>
<p>The concept of the avatar, originating from the Sanskrit "avatāra," meaning the descent of a deity into the earthly realm, signifies the user’s ability to project themselves into a digital space, effectively "descending" into a virtual reality. This process of embodiment allows the user to interact within the environment, engage with other avatars, and navigate the complexities of virtual worlds.</p>
<p>Avatars serve not merely as tools of interaction but as extensions of the self, often embodying aspects of the user’s personality or <<define "persona" "Persona">>. In some contexts, the avatar functions as a mask, concealing the user’s true identity, while in others, it reveals and amplifies certain characteristics, allowing for a form of self-expression that might be limited in the physical world.</p>

:: multimedial [definition nobr]
<p>A multimedial text is a text which exists on and across multiple platforms. In the era of "Big Marvel," this can be compared to the corporate world's zest for vertical integration, but the term ought to be untethered from capitalism's totalizing yoke and instead connected with texts, like <i>Achewood</i>, which exist online, on paper, in games, and, in short, across multiple <<define "media" "medium">> forms. A multimedial text may or may not be <<define "heterogeneric," "heterogeneric">> but the terms are not equivalent. Although different media forms do lend themselves to different genres, the two terms are distinct inasmuch as 'media' focuses on platform and the materiality of the text--as film, as game, and paper book--and 'genre' focuses on the tropes, dramatic structure, and other distinguishing features of the text.</p>

:: medium [definition nobr]
<p>Medium, often seen in this study in its plural "media" form, describes the technological substrate on which a given text is transmitted or appears. A text can be <<define "multimedial">>, in which case it appears on multiple media, or it appear via a single medium, like a paper book (although, even in this case, a book's <<define "paratext" "paratexts">> might well cross over into other media.</p>
<p>Marshall McLuhan's famous dicta that "the medium is the message" and that a medium can be defined as a "extension of ourselves" gestures towards the co-constituitive relationship that media, <<define "genre">>, and <<define "identity and selfhood" "selfhood">> have in contributing to personal identity in the <<define "information age">>.</p>

:: Persona [definition nobr]
<p>The word “person” has an interesting history. On the one hand, there’s the common understanding of it, which the Oxford English Dictionary lists as the second entry: “an individual human being; a man, woman, or child.” On the other, it stems etymologically from the Latin persona, meaning “mask used by a player, character in a play, dramatic role, the part played by a person in life, character, role, position,” which gives “person” its first entry: “a role taken by a person.”  Most intriguing is “person”s sister word, persona, which remains in English and means “the aspect of a person’s character that is displayed to or perceived by others” <<cite "OED" "Oxford English Dictionary">>. These terms suggests two ways of understanding personhood: in one, the subject is constituted by their individuality, their mind, their personality – in a word, their self. In the other, a person is constituted by the way their identity is performed and perceived by other people – a sort of constructed social identity made up by the masks they wear. The former understanding presumes an autonomous subject and an autonomous will in a way predicated on a kind of essentialism; the latter is more in line with the insights provided by the postmodern turn, which has decentered the subject in radical and productive ways.</p>

<p>This decentered subject has been the object of critical study for decades now. Judith Butler, in particular, has provided insight into the way a subject is constituted by their iterated <<define "performative acts" "information and selfhood">>, the organ of which is the body. Her argument is compelling, but as we migrate to new, digital worlds, it makes less sense to privilege the <<define "material body" "meatspace">> over the <<define "avatar" "Avatar">> as the instrument with which one plays oneself.</p>

:: Section 107 of the Copyright Act [definition] {"position":"4194,2268","size":"100,100"}
"107. Limitations on exclusive rights: Fair use
Notwithstanding the provisions of sections 106 and 106A, the fair use of a copyrighted work, including such use by reproduction in copies or phonorecords or by any other means specified by that section, for purposes such as criticism, comment, news reporting, teaching (including multiple copies for classroom use), scholarship, or research, is not an infringement of copyright. In determining whether the use made of a work in any particular case is a fair use the factors to be considered shall include—

(1) the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes;

(2) the nature of the copyrighted work;

(3) the amount and substantiality of the portion used in relation to the copyrighted work as a whole; and

(4) the effect of the use upon the potential market for or value of the copyrighted work.

The fact that a work is unpublished shall not itself bar a finding of fair use if such finding is made upon consideration of all the above factors."

:: data into information [definition] {"position":"3721,5103","size":"100,100"}
The most naive way of expressing the distinction between <<define "data" "Data">> and <<define "information" "Information">> is to rely on Luciano Floridi's definition of information as "meaningful, well-ordered data" <<cite "8" "Floridi, Luciano. Information: A Very Short Introduction.">>. Data here are the raw material out of which instrumentally useful information is made. In other words, as Lisa Gitelman and Virginia Jackson put it in the introduction to <i>'Raw Data' is an Oxymoron</i>, "data are apparently before the fact" <<cite "2" "Gitelman, Lisa. 'Raw Data' is an Oxymoron.">>.

Instrumentally, then, we can think about data as 'preprocessed' information that needs to be mined and refined until it's meaningful, like the raw sense data of Kantian cognition that precedes the operations of the imagination and the understanding. Zeroes and ones, serialized dictionaries, and electrical variances become lights on a screen that make words, images, and other <<define "medium">> with meaning; the former is data, the latter information.

However, as Gitelman's title suggests, this definition fails to account for the way in which data have to be "imagined <i>as</i> data to exist and function as such, and the imagination of data entails an interpretive base" <<cite "3" "Gitelman, Lisa. 'Raw Data' is an Oxymoron.">>--and, as research and news stories on everything from cameras that can't see people of color to algorithms that re-enact redlining have demonstrated, an interpretive base is awfully close to an interpretive bias. Data cannot be prior to interpretation because its extraction is accomplished by human-made tools that can reflect the biases, blind spots, and bad motives of their engineers. Garbage in, garbage out.


:: Glitch [definition nobr] {"position":"3719,2968","size":"100,100"}
<p>The <span class="glitch" data-text="glitch">glitch</span> here is defined as both a technical term and as an aesthetic moment.</p>

<p>From a technical perspective, a <span class="glitch" data-text="glitch">glitch</span> is, per the OED:</p>
<blockquote>A surge of current or a spurious electrical signal; also, in extended use, a sudden short-lived irregularity in behaviour.<<cite "*" "Oxford English Dictionary">></blockquote>

<p>From an aesthetic perspective, however, the <span class="glitch" data-text="glitch">glitch</span> is a moment when less becomes more, when the <<define "lossy" "loss">> contingencies of digital being paradoxically give rise to the production of something novel. This is often accompanied by a shift in the hermeneutic categorization of <<define "signal" "Signal">> and <<define "noise" "Noise">> as discrete; instead, under the sign of the <span class="glitch" data-text="glitch">glitch</span>, the two switch places, intermingle, and become a new signal.</p> 

<p>In the context of informational performativity, we might think about the <span class="glitch" data-text="glitch">glitch</span> as a mode of resistance to binary oppositions imposed by platforms.</p>

<p>See [[Blue Kettle->Case Study: BLUE KETTLE]] and [[Menkman's Glitch Studies Manifesto->Ross Menkman’s 2010 Glitch Studies Manifesto]].</p>

:: transcoding [definition todo] {"position":"3719,2568","size":"100,100"}
"In new media lingo, to 'transcode' something is to translate it into another format. The computerization of culture gradually accomplishes similar transcoding in relation to all cultural categories and concepts. That is, cultural categories and concepts are substituted, on the level of meaning and/or the language, by new ones which derive from computer’s ontology, epistemology and pragmatics. New media thus acts as a forerunner of this more general process of cultural re-conceptualization." <<cite "45" "Manovich, Lev. The Language of New Media.">>


:: Loss [definition nobr] {"position":"3602,2569","size":"100,100"}
<p><<define "Data">> takes up space to store and bandwidth to transmit. By utilizing compression algorithms, the amount of space that data takes up can be reduced. However, that compression is seldom a lossless--or reversible--process. Instead, data is ordinarily lost during compression. These compression algorithms undergird the systems by which <<define "medium">> is transmitted online. </p>
<p>Loss is not unique to digital transcoding. Consider the process by which a 70mm film is transcoded into a digital format. This process can be 'lossless' inasmuch as it produces an enormous video file that, in theory, could be used to print a new 70mm film. However, as the protestations of film-first afficionadoes and vinylheads attest, analog media is continuous where digital media is not; nuances are often lost.</p>
<p>In his memoir, Brian Eno writes about the nostalgic premium that accompanies lossy media, and how that <<define "noise" "Noise">> can itself become the <<define "signal" "Signal">> that the user seeks out:</p>
<blockquote>
	Whatever you now find weird, ugly, uncomfortable and nasty about a new medium will surely become its signature. CD distortion, the jitteriness of digital video, the crap sound of 8-bit — all of these will be cherished and emulated as soon as they can be avoided.<br>
	It’s the sound of failure: so much modern art is the sound of things going out of control, of a medium pushing to its limits and breaking apart. The distorted guitar sound is the sound of something too loud for the medium supposed to carry it. The blues singer with the cracked voice is the sound of an emotional cry too powerful for the throat that releases it. The excitement of grainy film, of bleached-out black and white, is the excitement of witnessing events too momentous for the medium assigned to record them. <<cite "283" "Eno, Brian. A Year With Swollen Appendices.">>
</blockquote>

:: Unity [definition] {"position":"3594,3568","size":"100,100"}
<a target="_blank" href="https://unity.com/">Unity</a> is a popular modern game engine that stands out for its flexibility. A single project created in Unity can generally be exported to many different platforms -- Mac, Windows, Android, Web, VR, and others. Creators use a combination of the programming language C# and Unity's GUI to create their game. I have used Unity to create [[EndlessGame->Case Study: ENDLESSGAME]] and [[Almost to the Tick->Case Study: ALMOST TO THE TICK]].


:: virtual reality [definition nobr gpt]
<p>The term "virtual reality" (VR) was coined in the 1980s by Jaron Lanier, one of the earliest developers of VR technology and the founder of VPL Research, a now-defunct company that was instrumental in the development of VR technology. He defined it as "a computer-generated simulation of a three-dimensional image or environment that can be interacted with in a seemingly real or physical way by a person using special electronic equipment" <<cite "Lanier" "Lanier, Jaron. Virtual Reality.">>. This definition is still in use today, although it has been expanded to include non-computer-generated simulations, such as those produced by 360-degree cameras.</p>

<p>Lanier's definition is helpful in understanding what VR is, but it is also important to understand what VR is not. VR is not, as is often assumed, simply a computer-generated simulation of reality; it is a simulation that is designed to be interacted with in a physical way. This physicality is essential to the VR experience; without it, VR would simply be a three-dimensional movie. The interaction can take many forms, but it must be more than simply looking at the simulation; it must be an embodied experience. This is why VR technology is often referred to as "immersive technology."</p>

<p>The term "virtual reality" is sometimes used interchangeably with "augmented reality" (AR), but the two are not the same. AR is a technology that superimposes computer-generated images on the real world, while VR is a technology that replaces the real world with a computer-generated simulation. AR is often used for navigation, while VR is often used for entertainment.</p>

<p>Most versions of the metaverse take place in a kind of VR.</p>


:: Motion-Capture Data [definition nobr gpt]

<p>"Motion-capture data" refers to the digital information captured and recorded from real-world movements and actions, typically using specialized technology known as motion-capture systems. These systems utilize various sensors, cameras, and markers to track and record the motions of objects or living beings, such as humans or animals, in order to create accurate and realistic representations of movement in virtual environments.</p>
<p>The process of capturing motion involves placing markers or sensors on key points of the subject's body, which are then tracked as the subject moves. High-speed cameras or other sensor technologies record the spatial positions and orientations of these markers over time, generating a dataset that represents the intricate details of the subject's motion. This data can then be processed and translated into a digital format that can be applied to virtual characters, models, or animations.</p>
<p>Motion-capture data has a wide range of applications across various industries, including entertainment, sports, medicine, and research. In the entertainment field, motion-capture technology is frequently used in filmmaking and video game production to create lifelike character animations. Actors wearing motion-capture suits perform actions, and their movements are captured and mapped onto digital characters, enhancing realism and immersion in visual media.</p>
<p>In sports, motion-capture data is used to analyze athletes' movements, helping coaches and researchers study techniques, improve performance, and prevent injuries. In medical settings, motion-capture technology aids in assessing patients' physical movements and tracking their progress during rehabilitation and therapy programs. Additionally, researchers use motion-capture data to study biomechanics, cognitive processes, and human behavior in diverse fields of study.</p>
<p>One of the challenges associated with motion-capture data is the sheer volume of information generated during the capture process. Raw motion-capture data may require processing to remove noise, calibrate measurements, and align data across multiple sensors or cameras. Moreover, accurately applying the captured data to virtual models or characters requires specialized software and techniques to ensure realistic and seamless animation.</p>
Motion capture http://mocap.cs.cmu.edu/ . This passage requires expansion.

:: cognition [definition nobr]

<p>For N. Katherine Hayles, cognition can be defined as "the ability to sense information from the environment, interpret it, and act in contexts that connect it with meaning" <<cite "*" "Hayles, N. Katherine. The end of being human.">>.This definition is expansive, deliberately encapsulating processes performed by both biological and technological actors. Specifically, Hayles extends cognition beyond the biological and describes how humans and culture more broadly are "forming strong symbiotic bonds with computational media" <<cite "*" "Hayles, N. Katherine. The end of being human.">>.
<p>Hayles' definition is also pragmatic, echoing the Wittgensteinian maxim that the meaning of a word is its use in a language. Active cognitition turns the raw material of the noumenal world into information rendered meaningful inasmuch as it drives actions in context.</p>
<p>This broad definition of cognition is articulated as a spectrum, from "high cognition" like abstraction, language, and mathematics to "low cognition" like pattern recognition. As posthumans in the Haylesean sense of the term, humanity has moved towards exteriorizing nonconscious, low cognition to technological apparatuses, giving rise to <<define "cognitive assemblages" "cognitive assemblage">>. This argument echoes Clark and Chalmers' discussions of <<define "the extended mind">>. In both cases, biological and technological systems enmeshed in cybernetic, nonconscious loops of greater or lesser complexity embody and <i>produce</i> consciousness.</p>

:: cognitive assemblage [definition nobr]

<p>A cognitive assemblage is N. Katherine Hayles' term for what Chalmers and Clark call <<define "the extended mind">>. In her words, cognitive assemblages are:</p>

<blockquote>tighter or looser collections of technical objects and human participants through which <<define "cognitions" "cognition">> circulate in the form of information, interpretations, and meanings <<cite "3" "Hayles, N. Katherine. Writing Posthuman: The Literary Text as Cognitive Assemblage.">>.</blockquote>

<p>Hayles differs from Chalmers and Clark in focus, among other things; for her, interpretation is central, and meaning is "oriented towards a pragmatist sense of the consequences of actions" <<cite "11" "Hayles, N. Katherine. Writing Posthuman: The Literary Text as Cognitive Assemblage.">>.</p>

:: the extended mind [definition nobr]
<p><i>From [["The Extended Mind in Krapp's Last Tape"->Krapp 5]]</i></p>
<p>In their 1998 paper 'The Extended Mind', Andy Clark and David Chalmers address the boundary between the mind and the rest of the world and ultimately advocate for what they call "active externalism," a way of thinking about the mind that acknowledges the active role of the environment, including technological devices, in driving cognitive processes <<cite '2' 'Clark, Andy and Chalmers, David. "The Extended Mind".'>>.</p>
<p>For Clark and Chalmers, a student doing long division by hand is not just using the pen and paper as a tool; some of the work of <<define "cognition">> is actively platformed on the page. Because the mind is the seat of cognition, it makes sense to think about that student’s mind as extending beyond his skin and onto the page. In cases like these:</p>
<blockquote>the human organism is linked with an external entity in a two-way interaction, creating a coupled system that can be seen as a cognitive system in its own right. All the components in the system play an active causal role, and they jointly govern behaviour in the same sort of way that cognition usually does. If we remove the external component the system's behavioural competence will drop, just as it would if we removed part of its brain. Our thesis is that this sort of coupled process counts equally well as a cognitive process, whether or not it is wholly in the head.<<cite '8' '9' 'Clark, Andy and Chalmers, David. "The Extended Mind".'>></blockquote>
<p>This <<define "cybernetic entity" "cyborg">> can be compared usefully to Hayles' <<define "cognitive assemblage">>.</p>

:: Memory [definition nobr]
<p>The word "memory" is derived from the Latin <i>memoria</i>, meaning "recollection, memory, remembrance; a thing worth remembering, a memorial; a tomb or monument" <<cite "OED" "Oxford English Dictionary">>. The root of this word, <i>memor</i>, means "to remember, to call to mind; to mention, record, celebrate in memory; to be mindful of; to keep in mind, to keep in view" <<cite "OED" "Oxford English Dictionary">>. Memory, then, is the act or faculty of remembering, or the power of retaining knowledge acquired in the past, and is often used interchangeably with "recollection." </p>

<p>The word "memory" is also used to describe the physical apparatus through which we remember. This usage is derived from the Latin <i>memoria</i>, meaning "a store-house, a repository; a place where something is deposited for safe keeping" <<cite "OED" "Oxford English Dictionary">>. In this sense, memory is a storehouse of information, and the act of remembering is the act of retrieving this information. This usage is common in computing, where data is stored in memory – in RAM, for example – and retrieved when needed. Indeed, the word "memory" is often used to describe the capacity of a computer to store information.</p>

<p>The word "memory" is also used to describe a record of the past, either in the form of an account or a physical object. This usage is derived from the Latin <i>memoria</i>, meaning "remembrance, memory; a thing worth remembering, a memorial; a tomb or monument" <<cite "OED" "Oxford English Dictionary">>. In this sense, memory is a way of preserving the past, and the act of remembering is the act of keeping this record alive.</p>

:: Cognitive Loops [definition todo]
Define this!

:: content [definition nobr]
<p>"Content" is a catch-all term that reduces news articles, stories, videos, podcasts, and other forms of media to a single stream of input. The shift from genre-specific terminology to totalized "content" is a consequence of the social media age, where everything can appear as a link on an endless feed.</p>

:: Kant's Glitch [definition todo]

For Kant, power of judgment depends on the cognitive faculties: the sensibility (which is subserved by imagination) and the understanding. A brief digression: in the Critique of Judgment, Kant states that the two cognitive faculties are the imagination and the understanding; elsewhere, they are sensibility and the understanding, and the imagination is included in the sensibility. Because those two faculties are the ones in play for apprehension of the beautiful through a judgment of taste, this study will focus primarily on them. That is: when I refer to the cognitive faculties, I mean imagination and understanding. Sensibility is, more or less, the way we take in particulars from the world. This intuitive appropriation of the world itself is not governed by concepts yet – it’s just the raw input, before we’ve made any sense of it. The imagination synthesizes the raw input received from the sensibility into something like an image (or, in Kant’s words, “combines the manifold of intuition” (62)), which then gets passed over to the understanding. The understanding is the “law-giver,” which takes the processed image from the imagination and subsumes it under concepts, including the categories of the understanding (or, again in Kant’s words, provides “the unity of the concept uniting the [component] presentations (62)). There is some confusion here, depending on what Kant you’re reading: sometimes, it seems like the imagination synthesizes the input of the sensibility by applying the categories of the understanding; sometimes, it seems more like the imagination is the step just before the understanding, which gives the understanding the picture that the understanding makes sense of. I will use an example to illustrate the process as I understand ig.
For example, let’s take a chair. When I look at the chair, some stuff happens with light and my eyes are barraged with sensory input. This gets brought into my mind through the sensibility, which includes intuition. My imagination takes those particulars and synthesizes it into something that the understanding can work on: I have the image of the chair, but it hasn’t yet resolved into anything that makes any kind of sense. Think of it like letters to a dead language: the letters is there, but they mean anything until the understanding works on it. In normal cognition, the chair is then subsumed under concepts, especially the categories of the understanding – transcendental concepts which structure phenomenal reality. This is when the dead language starts to make words. I can make out the shape of the chair, I see that it resembles other chairs, and so on. The object of my perceptions is subsumed under concepts and becomes an intelligible element of my phenomenal existence. This is, in brief, how normal cognition works. And sometimes we like things that we cognize regularly: through the “taste of sense,” (58) we arrive at pleasure, and as a result we find the object agreeable (although, importantly, the agreeable is not universal and instead “everyone has his own taste” (55)).
The beautiful is something very different. It is never subsumed under a concept: indeed, this is one of the defining characteristics of the beautiful. It is also always disinterested. Cognition begins as normal: we receive sensory input through the sensibility. However, things change when the faculty of the imagination receives those particulars: rather than working in conjunction with the understanding to synthesize the image and subsume it under concepts, during a judgment of taste (how we arrive at the beautiful) intuitions are not subsumed under concepts. Instead, the imagination in its freedom from concepts “harmonizes with the understanding in its lawfulness” (151). Kant calls this harmonization “free play,” and states that “a judgment of taste must rest upon the pure sensation of both the imagination in its freedom and the understanding with its lawfulness, as they reciprocally quicken each other, i.e. upon a feeling that allows us to judge the object by the purposiveness that the presentation has insofar as it furthers the cognitive powers in their free play” (151). Pleasure in the sensation of the free play between imagination and understanding is the subjective experience of purposiveness in the presentation of an object – not the object itself – without any purpose and hence as “the mere form of purposiveness” (66). This pleasure is beauty: “Beauty is an object’s form of purposiveness insofar as it is perceived in the object without the presentation of a purpose” (84).
Another example might be helpful. Let’s take a rose. A woman sees, breathes deeply around, and touches this rose. She receives, through her senses and her sensibility, the raw data of the rose – that stuff goes into her mind. Her imagination is there to catch that data and start to synthesize it into something. So far, so good: normal cognitition is at play, and we’re at the same place here as we were with the chair. But her imagination, playing with the bits and pieces of ununified particulars she’s supposed to make into something – say, a chair – is baffled by the configuration of the bits. She goes to the understanding for help – normally, the imagination is in a reciprocal, collaborative role with the understanding in cognition and it’s difficult to fully separate the two – and the understanding reaches for the concepts. You can think about these concepts like measuring implements that allow the understanding to turn raw data into the phenomenal world. But something happens. The concepts can’t subsume the particulars; the rose cannot be measured (or, at least, some dimension of the rose cannot be measured). After a moment of trying, the understanding drops her ruler and starts just playing a game with the imagination. And instead of getting “rose,” the woman in whom all of this anthropomorphized cognition is taking place gets the sensation of free-play, which is beautiful. The rose is beautiful because, when the woman looked at it, it sent her imagination and her understanding into free play with one another, and so she experienced the sensation of beauty. 
It’s crucial to note here where ‘beauty’ is. On the one hand, beauty seems to be tied up in the object inasmuch as there are beautiful objects. However, Kant repeatedly says that the experience of beauty is subjective – or, at least, that the reflective judgment on the experience of free play which makes the mind aware of pure purposiveness, which is to say beauty, takes place in the subject. The apparent contradiction resolves, more or less, when we keep in mind the cognitive process outlined above: the object spurs the imagination and the understanding into free-play, but the free-play is the sensation of beauty. The sensation of beauty inheres in the subject, even if the object is beautiful. And the judgment of taste that results in beauty is always reflective: it is not a function of the object but instead of reflection upon the sensation of freeplay. This is where we get the “subjective” part of the definition of beauty as the subjective universal. 
But what about universality? Kant says repeatedly that beauty is “what, without a concept, is liked universally” (64). Some other things are universal in Kant’s system; the moral law, for example. If a thing can be made universal through the operation of reason, then that thing is objectively – or logically – universal. However, the tie between ‘universal’ and ‘objective’ is a common one. ‘Subjective’ is a byword for non-universal, conditional, all in the head of the subject and not tethered to the real universe, so “subjective universal” seems to be a contradiction. Beauty is clearly subjective inasmuch as the experience of beauty, and reflection on that experience which yields an awareness of pure purposiveness, takes place within the subject. How can something be universal that takes place within the subject?
Kant answers this question by appealing to what he calls a common sense. On the one hand, Kant says that a common sense is “the effect arising from the free play of our cognitive powers” (87) – which sounds an awful lot like the sensation of the beautiful. But that’s tautological – it doesn’t get us much deeper in? The translator’s note is again helpful here: it states that a “common sense” is the presupposition, a priori, that “taste and the feeling by which it judges is common to everyone” (lv). Because the liking of the beautiful is “our consciousness of... the form’s purposiveness for that harmony” (lx) and because the harmony “can be assumed to be the same for everyone,” we can assume that taste is grounded on a common sense. Even this argument has a ring of circularity to it, though. The necessary addition to the system – the final piece to this specific puzzle – is the notion of universal communicability, on which common sense and the possibility of a subjective universal depends.
Common sense is “the necessary condition of the universal communicability of our cognition, which must be presupposed in any logic and any principle of cognitions that is not skeptical” (88) and that “only under the presumption of such a common sense, I maintain, can judgments of taste be made” (87). Common sense is thus grounded on the idea of universal communicability and is a necessary element of the system if we are to escape from skepticism. But how does Kant justify universal communicability – beyond simply rejecting the skeptical hypothesis by appealing to experience, which isn’t his style? First of all, the bulk of Kant’s rejection of skepticism is in the first Critique; bootstrapping out of Descartes is beyond the scope of this paper. Kant assumes on this basis that we can communicate our empirical cognitions and our judgments with other people, and that we are understood when we do so. And because we can communicate our cognitions and our judgments, we can assume that “the cognitive powers presupposed by this common understanding are shared universally as well” (lx). In other words: we assume cognitions can be communicated because we are not skeptical. In order for us to be able to communicate cognitions,  the “attunement of the cognitive powers required for cognition in general” – the functional relationship between them that turns the particulars of the outside world into phenomenal reality / cognition – must also be universal communicable. This universally communicable functional relationship between the cognitive powers is the subjective condition of the process of cogntion, and is necessary in order for us to have cognition (88). Kant restates this later on in the Critique, approaching the question from a slightly different angle:
“The aptitude that human beings have for communicating their thoughts to one another requires that imagination and understanding be related in such a way that concepts can be provided with accompanying intuitions, and intuitions in turn with accompanying concepts, these intuitions and concepts joining together to form cognition... only where the imagination is free when it arouses the understanding, and the understanding without using concepts puts the imagination into a play that is regular, does the presentation communicate itself not as a thought but as the inner feeling of a purposive state of mind.” (163)
The attunement of the cognitive powers (imagination and understanding) varies depending on the objects given: imagination and understanding behave differently when they’re processing a chair than when they’re processing a jar of marmalade, for example. But because cognitions can be communicated, we can assume that the mental state of taking in marmalade and the mental state of taking in a chair – the attunement of the faculties which gives rise to the phenomenal world of chair or marmalade – is also communicable. 
Why does this mean that everyone ought to judge the same things to be beautiful? Beauty is a special case because, as we’ve mentioned, it does not depend on concepts. According to Kant, there is “one attunement” – one particular kind of input – “in which this inner relation is most conducive to the (mutual) quickening of the two mental powers with a view to cognition (of given objects) in general.” This is determined by feeling, rather than concepts: this “one attunement” is the experience of the beautiful. Because it is still an attunement of the cognitive faculties, which we’ve already proven are universally communicable if cognitions are to be communicated at all, it is universally communicable. It depends not on the individual, but on the sensation of the attunement of the imagination and understanding – free play. 
The imagination and the understanding interact with one another by a set of rules, grounded in the constitution of the mind – “an original contract dictated by our very humanity” (164), and because this rule is standard and valid for every human, the same input going into the same cognitive apparatus will produce the same sensation of beauty. This is necessary for cognition to be universally communicable and is the ground of the possibility of beauty as a subjective universal. It is subjective because the feeling of beauty inheres in the subject rather than in the object; it is universal because attunement of the understanding and the imagination are bounded by the same parameters in every human, and the free play of those cognitive faculties is a common sense to everyone: the sensation of pure purposiveness (Kant calls this the “universality of the subjective conditions for judging objects” (62)). This is why Kant can say that the person who proclaims something to be beautiful “requires the same liking for others” and judges “not just himself but for everyone” (55). In other words, perhaps dissatisfyingly, Kant states that “We must be entitled to assume a priori that a presentation’s harmony with these conditions of the power of judgment is valid for everyone” (155) because we’re able to talk to each other.
But people clearly do not always agree on what is beautiful. This is not a contradiction, according to Kant, because of the specifically conditional nature of the injunction towards agreement that the beholder of the beautiful posits. Kant states that “whoever declares something to be beautiful holds that everyone ought to give his approval to the object at hand and that he too should declare it beautiful” (86). The ought here, in an aesthetic judgment – a judgment about the beautiful – is merely conditional, and “the common sense... is a mere ideal standard” (89).  If I think this mountain is beautiful, it’s because I take in the mountain (which is common to everyone) through my sensibility (which is common to everyone) and then my imagination and understanding get quickened into free play (the conditions of which are common to everyone, according to common sense). 
Hence “we solicit everyone else’s assent because we have a basis for [the aesthetic judgment] that is common to all” (86). However, because we can’t be sure that instance – here, the mountain – has been “subsumed” – it’s possible that someone else could not experience the mountain as beautiful. What is on trial is the claim that beauty has on everyone’s liking, not the liking itself, because the liking itself could be conditioned by exterior factors like interest. Presuming that taste is deployed properly, that the subjects in question are properly disinterested, the cognitive faculties are harmonizing with one another properly, and the concepts are not in play (for concepts of the object cannot be involved in a judgment of the beautiful) the subject who judges something to be beautiful “cannot help judging that it must contain a basis for being liked [that holds] for everyone” (54).
Because taste isn’t always deployed properly, if we want universal agreement on questions of the beautiful we still have a role for the critic, who should “investigate our cognitive powers and what these tasks perform in these judgments” and “clarify by examples the reciprocal subjective purposiveness” whose form in a presentation is “the beauty of the object of this presentation” (150). The critic’s job is made more difficult because the beautiful cannot be subsumed under concepts of the object. Consequently, “there can be no rule by which someone could be compelled to acknowledge that something is beautiful” (59). There’s a tension here: on the one hand, the critic can help us figure out our cognitive powers, understand purposiveness, and become disinterested; on the other hand, because beauty is always subjective as well as being universal, we always want to submit the object to our own eyes – indeed, we have to. 
So taste is autonomous, because it is subjective, but the object of taste – the beautiful – is objective. We can talk to one another about taste – it is “precisely what stands most in need of examples regarding what has enjoyed the longest-lasting approval” (147) and in doing so hone our taste, make ourselves more receptive to the proper attunement of the cognitive faculties that is beauty. But, for Kant, the beautiful is only of empirical interest in society. Taste in society is “an ability to judge whatever allows even our feeling to everyone else” and is hence “a means of furthering something that everyone’s natural inclination demands [i.e., sociability]” (163). Because beauty is the reflective awareness of the pure purposiveness in free play, freedom, harmony, and so on, and because that feeling of beauty is the same in all people, beauty drives us to seek out others who agree with us. We claim that they should be able to because if they can feel what we can feel – and they can, if only their taste is operating correctly – we would have confirmation of our judgment of taste. Society hus operates as a sort of empirical testing-ground for our judgments of the beautiful. The beautiful confirms our place in society because it underlines the universal constitution of the human mind, with its drive towards free play, and the person who judges an object beautiful wants to share that with others.  


:: Algorithm [definition nobr gpt]
<p>An algorithm is a step-by-step procedure or set of rules designed to perform a specific task or solve a particular problem, often implemented through programming in computational systems. Algorithms form the backbone of digital technology, enabling machines to process data, make decisions, and execute actions automatically. They are foundational to the operation of everything from basic arithmetic calculations to complex artificial intelligence systems.</p>

<p>In the context of <<define "Database">> and computational culture, algorithms not only execute processes but also shape the ways in which information is organized, accessed, and interpreted, thus influencing the construction of knowledge itself.</p> 

<p>Within the scope of this dissertation, algorithms play a crucial role in examining how digital technologies mediate and often manipulate human perception and behavior. The dissertation argues that algorithms, particularly those embedded in recommendation systems and social media platforms, actively participate in shaping the self by selectively amplifying certain content while suppressing others. This curation of information contributes to echo chambers and the radicalization pathways discussed in the broader work, illustrating how algorithms are not neutral tools but active agents in the construction of digital realities.</p>

:: 'Mexican' Magical Realism [definition nobr]

<a href="https://achewood.com/2006/07/31/title.html"><img style="max-width:100%" src="assets/imgs/achewood_mexican_magical_realism.png" alt="Ray: Dude. You know what it is? This camera is showin' how you FEEL about yourself, not your actual self Ray: That's why Teodor looks like some blowjob from the Banana Republic ads, and Beef looks like a goon who lost his jack-off virginity in the bathroom at Comic-Con Ray: I ain't gonna say anything, but your camera has Mexican magical realism. Same thing happened to me last year when I bought that RV where it was always rainin' inside. [Ray shown in the interior of an RV while it rains inside] Teodor: What in the hell are you talking about? Ray: I ain't tell you about that? Thing was a total hassle. I ended up sellin' it to some friend I don't know anymore. Anyhow, turns out the thing was made in Mexico. I bet if you scratch the Made in China sticker off your camera, it says Hecho en Mexico dude. {{Briefly Thereafter}} Teodor: Wow, he's right! This camera was made in Mexico Roast Beef: Dude Teodor let's take pictures let's take pictures of everyone and see what they think of themselves man that would be the Dickens of a chuckle! Teodor: Wow. What if we took a picture of Pat? {{then}} [Pat wear leather chaps, holding a Chicken thigh and sausage links, in pure, unadulterated homosexual ecstasy]" title="Ray: Dude. You know what it is? This camera is showin' how you FEEL about yourself, not your actual self Ray: That's why Teodor looks like some blowjob from the Banana Republic ads, and Beef looks like a goon who lost his jack-off virginity in the bathroom at Comic-Con Ray: I ain't gonna say anything, but your camera has Mexican magical realism. Same thing happened to me last year when I bought that RV where it was always rainin' inside. [Ray shown in the interior of an RV while it rains inside] Teodor: What in the hell are you talking about? Ray: I ain't tell you about that? Thing was a total hassle. I ended up sellin' it to some friend I don't know anymore. Anyhow, turns out the thing was made in Mexico. I bet if you scratch the Made in China sticker off your camera, it says Hecho en Mexico dude. {{Briefly Thereafter}} Teodor: Wow, he's right! This camera was made in Mexico Roast Beef: Dude Teodor let's take pictures let's take pictures of everyone and see what they think of themselves man that would be the Dickens of a chuckle! Teodor: Wow. What if we took a picture of Pat? {{then}} [Pat wear leather chaps, holding a Chicken thigh and sausage links, in pure, unadulterated homosexual ecstasy]"></a>

:: magazine [definition nobr]
<p>A great deal of scholarly work has taken place on the subject of magazines, magazine culture, and gender. Magazines make for a particularly fertile field of gendered analysis for several reasons: as "low-brow" cultural items, they are often (though not universally, with the proliferation of, for example, feminist magazines, or the ballyhooed repositioning of <i>Teen Vogue</i> as a venue for hard-hitting investigative journalism) less self-consciously concerned with issues like gender construction; they have, as Elizabeth Groeneveld writes, "targeted readerships based on gender for almost the entire history of their publication" <<cite "214" "Groeneveld, Elizabeth. Gender in Magazines">>; and they are heterogenous and multimedia by definition, containing a collage of articles, laser-targetted advertisements, and aspirational imagery.</p>
<p>As noted Beckett scholar Jonathan Bignell writes in <i>Media Semiotics: An Introduction</i>, both men's and women's magazines can be read as primarily concerned with gender construction and performance. A look at the products advertised, for example, reveals that</p>
<blockquote>The largest categories of products advertised in women's magazines are cosmetics, clothes, and food... [and] the largest categories of products advertised in men's magazines are cosmetics, clothes, and cars.<<cite "58" "Bignell, Jonathan. Media Semiotics: An Introduction">></blockquote>
<p>The first two of these categories, clothes and cosmetics, are two of the most conspicuous modes through which individuals stylize their bodies into gendered performances. In other words, men's and women's magazines tell their readers what it means to be a good man or woman and then sell them what they need to fashion themselves according to that ideal.</p>


:: McGuffin [definition nobr] {"position":"1999,1306","size":"100,100"}
<p>A McGuffin (usually styled "MacGuffin") is a cinematic term that signifies a sort of plot device that is not in itself important -- for example, the Maltese Falcon or the briefcase full of gold in Pulp Fiction. The term was coined by screenwriter Angus MacPhail but popularized by Alfred Hitchcock:</p>
<figure>
	<video controls>
	<source src="assets/videos/macguffin.mp4" type="video/mp4">
	Your browser does not support the video tag.
	</video>
	<figcaption>Alfred Hitchcock explaining the term "MacGuffin" in a 1972 interview with Dick Cavett. <<cite "*" "Cavett, Dick. 'Alfred Hitchcock'">></figcaption>
</figure>
<blockquote>
It might be a Scottish name, taken from a story about two men on a train. One man says, "What's that package up there in the baggage rack?" And the other answers, "Oh, that's a MacGuffin." The first one asks, "What's a MacGuffin?" "Well," the other man says, "it's an apparatus for trapping lions in the Scottish Highlands." The first man says, "But there are no lions in the Scottish Highlands," and the other one answers, "Well then, that's no MacGuffin!" So you see that a MacGuffin is actually nothing at all.
</blockquote>

:: Love and Information [definition nobr] {"position":"5044,918","size":"100,100"}
<p><i>Love and Information</i> is a modular play written by Caryl Churchill. It first opened at the Royal Court Theatre in September 2012. I have adapted it into Twine <a href="assets/projects/lai_twine.html">here</a>; this adaptation is the focus of the third [[case study->Case Study: LOVE AND INFORMATION]] in this dissertation.</p>
<p>Please note that I expressly did <i>not</i> receive permission by the Churchill estate to create this adaptation (like my Beckett adaptations). however, because the work is scholarly, not-for-profit, transformative, and will have no impact on the sales of the original work, my usage constitutes <<define "Fair Use">> and is, I submit, defensible on those grounds.</p>

:: Ship of Theseus [definition nobr]
<p><span class="blink1">T</span><span class="blink2">h</span><span class="blink3">e</span> <span class="blink4">S</span><span class="blink1">h</span>i<span class="blink2">p</span> <span class="blink3">o</span><span class="blink4">f</span> <span class="blink1">T</span><span class="blink2">h</span><span class="blink1">e</span><span class="blink3">s</span><span class="blink2">e</span><span class="blink4">u</span><span class="blink1">s</span> is a <<define "thought experiment" "Thought Experiment">> about identity. In short, the thought experiment considers a ship that has some of its components replaced every year. At what point, the experiment asks, has the identity of the ship changed? It was memorably formulated by Plutarch:</p>
<blockquote>
The ship on which Theseus sailed with the youths and returned in safety, the thirty-oared galley, was preserved by the Athenians down to the time of Demetrius Phalereus. They took away the old timbers from time to time, and put new and sound ones in their places, so that the vessel became a standing illustration for the philosophers in the mooted question of growth, some declaring that it remained the same, others that it was not the same vessel.<<cite "*" "Plutarch. Lives: Theseus.">>
</blockquote>

:: emoji [definition nobr]
<p>In the context of digital performativity, emoji perform a particuarly interesting function. As iconic representations of facial expressions and other stylizations of the body on chat platforms that ordinarily lack a visual component beyond embedded images and a profile photo, the emoji should be read as a <<define "gesture" "information and selfhood">> in the performative sense.</p>
<p>Gretchen McCulloch has written that emoji function as emblems; that is, they have "precise forms and stable meanings" <<cite "161" "McCulloch, Gretchen. Because Internet.">> within particular contexts--and those meanings change when moved to different contexts. She argues that they contribute to digital embodiment by allowing people to provide tonal markers to plain text.</p>

:: V-Bucks [definition nobr]
<p>The precise dollar value of these cosmetic items is difficult to pinpoint because they are purchased with V-Bucks, which can be purchased with American dollars at an approximate rate of 100 V-Bucks per American dollar. Discounts are available with larger purchases, however; $24.99 will get you a ‘bonus’ 300 V-Bucks, and users can earn V-Bucks for performing in-game actions.</p>
<p>YouTuber Dan Olson, creator of the popular channel "Folding Ideas", has an excellent video on just this topic that also opens with a discussion of D.J. Marshmallow's concert but digs deeper into the way that Fortnite's economy as a whole is built to take advantage of its (mostly underage) players. In his words, "V-Bucks are used here, as in most other microtransaction games, as a means to bypass financial regulations involving held value and some regional laws regarding gambling... the use of V-Bucks allows [Fortnite publisher] Epic to obfuscate the assigned value of both in-game purchases and the value of V-Bucks themselves." <<cite "*" "Olson, Dan. 'Manufactured Discontent and Fortnite'.">></p>
<figure>
<iframe width="560" height="315" src="https://www.youtube.com/embed/dPHPNgIihR0?si=zKkkqxo5KvNDwuD5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<figcaption>Olson's "Manufactured Discontent and Fortnite", the whole of which is worth watching if you have the time.</figcaption>
</figure>
<p>In an unsurprising coincidence, the use of virtual currency also echoes <i>Snow Crash</i>’s two primary currencies, Kongbucks and Gippers. The proliferation of currency further estranges objects from their labor value.</p>

:: Caryl Churchill [definition nobr]
<figure><img style="max-width:50%" src="assets/imgs/caryl_churchill_sketch.jpg"><figcaption>A sketch of Caryl Churchill by the Wikimedia user "Petticonifer", from 2022.</figcaption></figure>
<p>Caryl Churchill is a British playwright whose work engages engages with themes of gender, power, technology, and language, among other things. She has written for the stage, for television, and for radio. In a 2011 poll by <i>The Village Voice</i>, 5 of the 20 dramatists polled rated her as the greatest liviing playwright<<cite "*" "Soloski, Alexis. 'Who is the greatest living playwright?'">></p>
<p>This study engages specifically and in an extended fashion with two of her works: [[Love and Information]], which has here been adapted to Twine, and [[Blue Kettle->Case Study: BLUE KETTLE]], one of the case studies in the first chapter. An excerpt [[From The Skriker]] was also inspirational in the early drafting of this document.</p>

:: Hugh Kenner [definition]
Hugh Kenner was a literary scholar whose work fused close reading with a prescient eye for media specificity. He wrote four books that featured Samuel Beckett in a major way: <i>Samuel Beckett: A Critical Study</i> (1961), <i>The Stoic Comedians: Flaubert, Joyce, and Beckett</i> (1962), <i>A Reader's Guide to Samuel Beckett</i> (1973), and (most importantly for my research) 1987's <i>[[The Mechanic Muse->Kenner, Hugh. The Mechanic Muse.]]</i>. His <i>The Counterfeiters: An Historical Comedy</i> (1968) also bears on the work of this dissertation.

:: Nick Montfort [definition]
Montfort, an associate professor of digital media at MIT in the Program in Writing and Humanistic Studies, sees himself as borrowing from the Oulipo tradition "a mathematical approach to literature, supplemented with contemporary technological capabilities and the language of programming" (Malecka et al 84).

:: paratext [definition nobr]
<p>My usage of "paratext" takes as its starting with Gerard Genette's definition, glossed eloquently in the paratext to his own book <i>Paratexts</i> by scholar Richard Macksey as:</p>
<blockquote>Those liminal devices and conventions, both within the book (<i>peritext</i>) and outside it (<i>epitext</i>), that mediate the book to the reader: titles and subtitles, psuedonyms, forewards, dedications, epigraphs, prefaces, interitles, notes, epilogues, and afterwords [...] but also the elements in the public and private history of the book, its [...] "public epitexts" (from the author or publisher) as well as "private epitexts" (authorial correspondence, oral confidences, diaries, and pre-texts. <<cite "xvii" "Macksey, Richard. Foreword. Paratexts.">></blockquote>
<p>In this study, "paratext" comes up primarily in the case of <i>Achewood</i>, where I use the term to refer to the blogs, cookbooks, and other ephemera that orbits around the central text of the comic strip. In the era of cinematic universes, "lore", and vertical integration, it is not atypical for a text to be outweighed by its own paratexts--indeed, it is difficult for it <i>not</i> to be, given how many authors have Twitter accounts and blogs of their own. When the ephemera in question has been written in the voice of characters from the primary text, however, something changes; because literary characters, like people more generally, are the accumulation of their words and deeds, a corpus of paratexts like what <i>Achewood</i> offers significantly broadens the characters themselves.</p>

:: genre [definition nobr]
<p>As a term that is related, but not identical, to <<define "medium">>, genre demands both a definitional and a functional approach. Two questions must be addressed: what is genre, and what does it do? One problem with the first question is the genre’s versatility. Even if we narrow our scope to the genres of literary texts, genre (from the Latin <i>genus</i>, meaning roughly “kind” or “type” but also “family” and “origin”) operates as a classificatory category on multiple levels of abstraction. “Novel” and “poetry” are both genres, but so are “science fiction” and “the Gothic,” and the difficulty in constructing a monolithic definition that accounts for these multiple valences of meaning reemphasizes the vexed nature of constructing organizational categories for diffuse objects like literary texts. </p>

<p>Derrida’s influential “law of genre” – that is, the ironic command that “genres are not to be mixed” – contains within itself its own impossibility. Much of his argument ends up deconstructing his own law by reading the “law of genre” under a series of different generic signs. His conception of genre as “a concept that is essentially classificatory and genealogico-taxonomic”, which relies on “the identifiable recurrence of a common trait by which one recognizes, or should recognize, a membership in a class” is surprisingly operative as a pragmatic starting point--with the caveat that “every text participates in one or several genres” and that “such participation never amounts to belonging" <<cite "55" "Derrida, Jacques. 'The Law of Genre'.">>.</p>

<p>The (or, more properly, a) genre of a text, then, is determined by the degree to which the text participates in those tropes common to those texts that belong to the genre in question – clear instances of the genre. For example, Robert Elliott, following Wittgenstein, states that one looks at “a number of satires about which there is no question – which are at the center of the concept, so to speak—and then decides whether or not work x has resemblances enough to the undoubted examples of the type to be included in it" <<cite "23" "Elliott, Robert C. 'The Definition of Satire'.">> In practice, we read similar texts, compare them with each other, and “derive through rational inference a temporary set.”</p>

<p>What genre <i>does</i> is just as complicated as what it <i>is</i>. Besides a generic categorization functioning as a <<define "paratext">>--we read Kurt Vonnegut's <i>Cat's Cradle</i> differently when we've picked up a pulp sci-fi edition than we do when we're reading the Norton scholarly edition--genre also has an <<define "epistemological function" "genre and epistemology">>.</p>

<p>Genre can also be thought of as analogous to gender; as Lauren Berlant puts it in <i>The Female Complaint</i>, “To call an identity like a sexual identity a genre is to think about it as something repeated, detailed, and stretched while retaining its intelligibility, its capacity to remain readable or audible across the field of all its variations <<cite "4" "Berlant, Lauren. The Female Complaint.">>.</p>

:: genre and epistemology [definition nobr]
<p>The <<define "genre">> of a text provides an interpretive framework, a discursive setting, and an epistemological posture for reading it. When E.D. Hirsch states that “all understanding of meaning is necessarily genre-bound”, he points to this constitutive role that genre-as-frame and <<define "paratext" "genre-as-paratext">> plays in providing the reader with a place to start. For Hirsch, “an interpreter’s preliminary generic conception of a text is constitutive of everything that he subsequently understands;” genre provides the “givens of a situation” which help suggest “a probable type of meaning” <<cite "14" "33" "Hirsch, E.D., from Validity in Interpretation.">>. Or, as Jonathan Culler says, a genre is “a set of expectations, a set of instructions about the type of coherence one is to look for and the ways in which sequences are to be read” <<cite "51" "Culler, Jonathan. 'Toward a Theory of Non-Genre Literature'.">> In other words, genre is always a provisional, participatory property of a text, which cues the reader that it (and its truth claims) ought to be read in a particular way.</p>

:: genre set [definition nobr]
<p>One could conceive, following this notion of <<define "genre">>, a computer program that utilized machine learning and a sufficiently large corpus of manually tagged texts in order to process a given text and spit out the degrees to which the text participates in those pre-existing genres. Working with multiple criteria – word choice, sentence structure, styles of narration – one could actually train a computer to make itself better at “reading” genre. See Ted Underwood's “Gender, Genre, and Point of View” for an example of how one might implement this kind of study.</p>

:: technology [definition nobr todo]
Greek etymology. Techne. Heidegger.

:: necromancy [definition nobr]
<p>Defined by the Oxford English Dictionary as "The art of predicting the future by supposed communication with the dead; (more generally) divination, sorcery, witchcraft, enchantment." Etymologically derived from the Latin "necromantīa" and the Greek "νεκρομαντεία", referring to the art of "predicting the future by supposed communication with the dead"</p>
<p>Necromancers are common in contemporary genre fiction and a staple villain of tabletop roleplaying games like <i>Dungeons and Dragons</i>. Despite occasional necromantic antiheroes like Tamsyn Muir's Harrowhawk from the <i>Locked Tomb</i> series, the art is generally a dark one; it is unsavory, and generally evil, to manipulate the dead to do your bidding.</p>
<p>While writing this definition, I found myself thinking of Professor John-Paul Riquelme, a member of my orals committee who would have been responsible for reading parts of this document had he not died in 2022. As the preeminent scholar of the Gothic, among many other things, I wondered what he'd say about necromancy. I had a dark thought: I could use <<define "technonecromancy">>, as I do in <i>36 Blackbirds</i>, to ask. And so I did; I fed ChatGPT4 a copy of Professor Riquelme's foreword to an overview of the 19th Century Gothic and asked it to write a definition of "necromancy" in his voice, knowing as I did so that there was something irreverent and maybe blasphemous about it but also feeling, rightly or wrongly, that it was an irreverence and a blasphemy that he might appreciate, given the last message I received from him was an admonition, on my dissertation, to "knock 'em undead'.</p>
<p>What came out was a failure, I think:</p>
<blockquote>Necromancy, a term rooted in the ancient interplay between death and forbidden knowledge, refers to the arcane practice of communing with the dead, typically to gain insight or wield otherworldly powers. This Gothic art, steeped in mystery and terror, embodies a paradoxical engagement with mortality, where practitioners seek to traverse the tenuous boundary between the living and the dead. The necromancer, often depicted as both sage and transgressor, conjures spirits and revives corpses, invoking a spectral confluence of life and death. Through its rituals, necromancy challenges the very fabric of natural law, unsettling the established hierarchy of existence and delving into the shadowy realms of human fear and fascination. This eldritch art, akin to the literary Gothic's conjoining of opposites, underscores the enduring human quest to master the unknown, revealing the darker impulses of our desire for control and understanding in a world haunted by the inevitability of death.</blockquote>
<p>There are cosmetic similarities to Professor Riquelme's style: an interest in doublings and the conjoining of opposites, for example, but something is incontrovertibly missing here. There is an echo and a ghost, but none of his wit or rigor. I cannot imagine Professor Riquelme defining a term without divining something provocative and playful in its etymology, without finding some pun, without tracing some allusion that would darkly illuminate the term.</p>
<p>And even where an echo is heard, it's difficult to know what I'm bringing to the hearing that echoes against what's actually there. Someone's hand is always moving the planchette.</p>

:: technonecromancy [definition nobr]
<p>I coin here the neologism "technonecromancy" as a portmanteau of <<define "technology">> and <<define "necromancy">> to refer to the practice of using <<define "large language models" "LLM">> like ChatGPT to channel and emulate the voices of the dead. In this document, the practice is primarily taken up in the collection of poetry <i>36 Blackbirds</i>, where I provide the same prompt to a language model and see what poems come out.</p>
<p>I draw specifically on "necromancy" here to evoke the unsavory, blasphemous, and maybe even immoral aspects of ventriloquizing the dead in this manner. This comes through especially strongly in the products proffered by Silicon Valley that purport to let those who grieve speak to their loved ones again; consider <a href="https://www.hereafter.ai/">HereAfter</a>, a startup that promises to let "loved ones hear meaningful stories by chatting with the virtual you" after you have died. The soul rebels against the productization of grief and of the desire for life after death manifest in this manner, and whatever chatbots can be wrung out of the combined writings of a grandmother seem, to me, to be fundamentally wrong.</p>
<p>However. Throughout this document, I have been open about my agreement with the existentialist premise that we are, fundamentally, what we do; for Sartre, writing in "Existentialism is a Humanism":</p>
<blockquote>The genius of Proust is the totality of the works of Proust; the genius of Racine is the series of his tragedies, outside of which there is nothing.</blockquote>
<p>If an author, in particular, can be taken to be the totality of what they have written, and that totality can be fed into a system that can alchemize out a voice which can produce new words which cannot easily be distinguished from what the author has actually written, and if we take this existentialist premise seriously, then there is a meaningful way in which technonecromancy <i>does</i> permit one to speak to the dead, if only in echoes.</p>
<p>Of course, the use of technology to hear from the dead is not particular to large language models. We do this when we listen to music recorded by now-dead musicians. Krapp does this when he plays back the young man he once was. The protagonist of <i>Her Story</i> trawls videotapes for hints of who her mother was. And so on.</p>

:: procrustean [definition nobr]

<p>The OED defines "procrustean" as "of, relating to, or resembling the practices of Procrustes; (hence) enforcing uniformity or conformity without regard to natural variation or individuality" <<cite "*" "Oxford English Dictionary">>. The Procrustes (Προκρούστης, roughly "the stretcher") of the definition was a Greek mythological figure who was famed for either stretching or cutting his victims' bodies so they could fit into an iron bed of fixed size:</p>
<blockquote>In Eleusis, moreover, he out-wrestled Cercyon the Arcadian and killed him; and going on a little farther, at Erineüs, he killed Damastes, surnamed Procrustes, by compelling him to make his own body fit his bed, as he had been wont to do with those of strangers. And he did this in imitation of Heracles. For that hero punished those who offered him violence in the manner in which they had plotted to serve him...<<cite "x" "Plutarch. Lives: Theseus.">></blockquote>
<p>The term has been generalized to refer to situations where arbitrary standards are imposed on heterogenous objects; for example, in computer science, a "Procrustean String" is a string of fixed length which is either padded with spaces or truncated to fit into an arbitrary length.</p>
<p>In this study, the term can be applied to several of the major terms under examination. <<define "Gender" "gender">> is the most obvious example; the gender binary is itself a procrustean bed which coerces its subjects to stretch or truncate themselves to fit into arbitrary categories--and, as a hermeneutic system, conditions the interpretive possibilities of the audience for whom the gendered subject performs. <<define "Genre" "genre">>, similarly, suits; consider Kurt Vonnegut's attitude towards the categorization of his own novels as science fiction:</p>
<blockquote>I have been a soreheaded occupant of a file drawer labelled ‘science fiction’ … and I would like out, particularly since so many serious critics regularly mistake the drawer for a urinal.<<cite "781" "Vonnegut, Kurt. Science Fiction.">></blockquote>
<p><<define "Medium" "medium">> might, too, be considered as procrustean; necessarily conditioned by the <<define "losses" "loss">> and particularities of its technological substrate, McLuhan's maxim might be rephrased to say that the medium must be massaged if it is to fit the meaning that it's supposed to.</p>
<p>All that said, Procrustes is killed by Thesues, and we do not need to lay in the beds we've been made to fit. Queer, <span class="glitch" data-text="glitch"><<define "glitchy" "Glitch">></span>, <<define "heterogeneric">>, <<define "multimedial">> texts and subject all exceed, with a greater lesser degree of ecstacy, the bounds of their beds.</p>

:: death of the author [definition nobr gpt]

<p>The Death of the Author is a concept introduced by Roland Barthes, challenging the traditional notion that the author's intentions and personal history are the ultimate authority on the meaning of a text. Barthes argues that the author's identity is irrelevant to the interpretation of the text, which should instead be understood as a multi-layered space where various voices and perspectives intersect. As Barthes puts it, "the author enters his own death, writing begins" <<cite "142" "Barthes, Roland. 'The Death of the Author'.">>. This perspective shifts the focus from the author to the reader, proposing that the true locus of meaning lies in the act of reading, where the reader becomes the site of convergence for the text's myriad possible interpretations.</p>
<p>This concept was influential in the development of poststructuralist thought, particularly in its rejection of authorial authority and its embrace of the text as a space of open interpretation. "Death of the Author" has since become a foundational idea in literary theory, particularly within the realms of deconstruction and reader-response criticism.</p>

:: UI [definition nobr]
<p>Working product designers (that is, the people who design the <<define "interface" "interfaces">> for apps, websites, and other interactive technological media) often distinguish between UI (user interface) and <<define "UX">> (user experience). As described in <<define "Figma">>'s resource library, UI refers to the "interactivity, look, and feel of a product screen or a website" <<cite "*" "UI vs. UX: What's the Difference Between UI & UX Design?">>. A UI designer might consider things like page layout, color scheme, font selection, and interactive elements. Realistically, a designer needs to be familiar with both UI and UX to create a good product. Figuratively, if a designer is like a writer, a UI "writer" would consider things like diction, character, plot, and punctuation to make a book enjoyable (or whatever else the book is meant to be).</p>

<p>For this dissertation, I had to think like a UI designer when selecting the colors, fonts, stylization of the different kinds of links, the presentation of videos and images, and so on.</p>

<p>I had help from my friend <a href="https://www.aliceclebeau.com/">Alice C. LeBeau</a>, who is a product designer by trade; over the course of creating this dissertation, I also became a professional product designer, which is a large part of my current full-time job.</p>

:: UX [definition nobr]
<p>Working product designers (that is, the people who design the <<define "interface" "interfaces">> for apps, websites, and other interactive technological media) often distinguish between UI (user interface) and <<define "UX">> (user experience). As described in <<define "Figma">>'s resource library, UX covers "a user's overall experience with the product or the website" <<cite "*" "UI vs. UX: What's the Difference Between UI & UX Design?">>. A UX designer considers how other products solve the problems that the product is meant to solve, the information architecture of the product, user feedback, and other things when designing a user experience. Figuratively, if a designer is like a writer, a UX "writer" would consider things like genre, market, audience, and the like to make sure a book does what it's supposed to do.</p>

<p>For this dissertation, I had to think like a UX designer when defining its information architecture (that is, how the different case studies could be accessed and how they were arranged), its navigation elements, the interactive elements in each case study, and how to respond to some of the different feedback I received.</p>

<p>I had help from my friend <a href="https://www.aliceclebeau.com/">Alice C. LeBeau</a>, who is a product designer by trade; over the course of creating this dissertation, I also became a professional product designer, which is a large part of my current full-time job.</p>

:: Figma [defintiion nobr]
<p>Figma is a software tool used by product designers to sketch out <<define "UX">> and <<define "UI">> specs which are implemented by software engineers to create interactive technological media like apps and websites. Most of this dissertation was initially in Figma and then implemented in Twine via a combination of TweeCode, Javascript, HTML, and CSS.</p>

:: diegetic [definition nobr]
<p>The term “diegetic” comes from the Greek διήγησις (diegesis), meaning literally “narration;” the noun, in English, has come to refer to narration or recitation. Diegetic music, then, is music that occurs as a part of the narrative.  A film’s characters can hear diegetic music. When <i>O Brother Where Art Thou</i>’s Soggy Bottom Boys take the stage and sing “In The Jailhouse Now” and the crowd sings along, the music is diegetic; another example comes from <i>Blood Simple</i>, where Maurice hits the jukebox and elicits the Four Tops’ “Same Old Song.”</p>
<p>Contrast with <<define "non-diegetic">> and compare to <<define "transdiegetic">>.</p>

:: non-diegetic [definition nobr]
<p>Non-diegetic sound is not a part of the narrative as experienced by the characters. The audience can hear it, but it doesn’t exist inside the world of the media.</p>
<p>Contrast with <<define "diegetic">> and compare to <<define "transdiegetic">>.</p>

:: transdiegetic [definition nobr]
<p>I have coined the term "transdiegetic" to describe a shift, in media, from <<define "diegetic">> to <<define "non-diegetic">> sound, or vice versa. The change is usually quick and seamless, which may account for why it’s so much easier to catch on second, third, fourth viewings.</p>
<p>The Coen brothers are directors who use this technique often. For example, when <i>Miller’s Crossing</i>’s Leo is in bed listening to his phonograph blaring “Danny Boy,” the music is diegetic and indeed impacts the narrative – were it not for the volume of the music, he would hear his bodyguard get pumped full of lead downstairs. The song remains audible after Leo has jumped out the window to chase his would-be assassins, even though the phonograph has presumably burnt up from the house fire. It even swells when he fires his machine gun at the mooks’ getaway car. “Danny Boy” thus undergoes a transdiegetic shift. It begins as diegetic music but becomes nondiegetic as the scene progresses. Another more subtle example occurs in <i>Raising Arizona</i>: the theme of the lullaby that Holly sings to Little Nathan recurs in the Biker of the Apocalypse’s non-diegetic theme.</p>
<p>Transdiegesis as a formal technique can subvert the traditionally voyeuristic role that a piece of media's reader/viewer/player has in relation to the characters; as an auditory analogue to the shot/reverse shot technique, transdiegetic moments often jar the player from one structure of identification into another.</p>
<p>In ludology, the term "hypermediate", in contrast to "immediate", has been used to describe the subjective experience of a transdiegetic moment <<cite "47" "Allan, Matthew. 'Doki Doki Subversion Club'.">>.</p>

:: mirror [definition nobr]
<div style="text-align: center;">
    <video id="cameraVideo" width="640" height="480" autoplay></video>
    <br>
    <button id="startCameraButton">Mirror</button>
    <figcaption>‘What do you mean you’ve been spying on me?’ she thought--hot, blind, unreasoning. ‘What do you mean you’ve been spying on me, with this thing in my hand that is an eye?’<<cite "*" "Lockwood, Patricia. The Communal Mind.">></figcaption>
</div>

<script>
document.getElementById('startCameraButton').addEventListener('click', function() {
    const videoElement = document.getElementById('cameraVideo');
    
    if (!videoElement) {
        alert('Video element not found!');
        return;
    }

    if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        navigator.mediaDevices.getUserMedia({ video: true })
            .then(function(stream) {
                videoElement.srcObject = stream;
                videoElement.play();
            })
            .catch(function(error) {
                alert('Error accessing the camera: ' + error.message);
            });
    } else {
        alert('Camera not supported on this device.');
    }
});
</script>

:: red pill [definition nobr]
TODO

:: Lacan [definition nobr]
<p>Neither of us need to spend any more time thinking about Jacques Lacan than we already have.</p>

:: Doki Doki Literature Club [definition nobr]
<p><i><a href="https://ddlc.moe/" target="_blank">Doki Doki Literature Club</a></i> (DDLC) is a visual novel / dating sim / horror game created by Dan Salvato and released for free in 2017. Like <i>[[Her Story->Her Story 1]]</i>, <i>DDLC</i> is a generically savvy game that takes advantage of its medium to highlight the porousness of the screen-world and the real world, with a particular emphasis on the <span class="glitch">glitch</span> and the gothic. Critics writing about <i>DDLC</i> have highlighted how the game's uncanniness and horror element manifest via game-breaking moments that break the fourth wall that Christopher Barkman describes as "metaleptic" <<cite "2" "Barkman, Chris. 'There’s no point in saving anymore': Diegesis and interactional metalepsis in Pony Island and Doki Doki Literature Club.">>. Matthew Allan, citing Florian Kerschbaumer and Tobias Winnerling, instead defines these moments as hypermediate, in contrast to the immediate, and notes how these moments of "gothic glitch break the player's immersion, making them aware of the medium as it shifts their attention from the immediate to the hypermediated"<<cite "47" "Allan, Matthew. 'Doki Doki Subversion Club'">>.</p>